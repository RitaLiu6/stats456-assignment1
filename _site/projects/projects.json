[
  {
    "path": "projects/2021-12-20-stockreturn/",
    "title": "Data Science Project: Predict Stock Return Using Machine Learnings Models",
    "description": "In this project, I and my teammates created and compared different machine learning models to predict the return of stocks in S&P 500 from 1999 to 2020. The models try to capture the fundamental analysis process.",
    "author": [
      {
        "name": "Rita Liu, Duc Ngo, Sivhuo Prak",
        "url": {}
      }
    ],
    "date": "2021-12-20",
    "categories": [],
    "contents": "\n\nContents\nIntroductionPaper Outline\nList of Variables\nLoading data\n\nData Preprocessing\nData Visualization\nLasso ModelBuilding Recipe\nSelect tuning parameter\nLasso results\nModel Evaluation\nInterpretable Machine Learning\n\nRandom Forest ModelBuilding Random Forest model\nRandom Forest results\nModel Evaluation\nInterpretable Machine Learning\n\nStacking ModelRandom Forest Candidate\nLASSO Candidate\nKNN Candidate\nStacking all candidates\nComparison of the three models\n\nStock Return Prediction for 2021\nConclusion\n\n\nhide\nlibrary(tidyverse)        # for data cleaning and plotting\nlibrary(tidymodels)       # for modeling ... tidily\nlibrary(lubridate)        # for date manipulation\nlibrary(openintro)        # for the abbr2state() function\nlibrary(gplots)           # for col2hex() function\nlibrary(RColorBrewer)     # for color palettes\nlibrary(ggthemes)         # for more themes (including theme_map())\nlibrary(plotly)           # for the ggplotly() - basic interactivity\nlibrary(gganimate)        # for adding animation layers to ggplots\nlibrary(transformr)       # for \"tweening\" (gganimate)\nlibrary(gifski)           # need the library for creating gifs but don't need to load each time\nlibrary(shiny)            # for creating interactive apps\nlibrary(janitor)  \nlibrary(stacks)            # for stacking models\nlibrary(glmnet)            # for regularized regression, including LASSO\nlibrary(ranger)            # for random forest model\nlibrary(kknn)              # for knn model\nlibrary(naniar)            # for examining missing values (NAs)\nlibrary(moderndive)        # for King County housing data\nlibrary(vip)               # for variable importance plots\nlibrary(rmarkdown)         # for paged tables\nlibrary(fastDummies)\nlibrary(usemodels)         # for suggesting step_XXX() functions\nlibrary(readr)\nlibrary(kableExtra)\nlibrary(DALEX)             # model Agnostic Language for exploration and explanation (for model interpretation)  \nlibrary(DALEXtra)\ntheme_set(theme_minimal())\n\n\n\nIntroduction\nThe financial market is a strange place that is very hard to navigate around. We have seen Warren Buffett, Ray Dalio, Charlie Munger - the very very best who have had billions of dollars. Others, 95% of the population, lose the money instead.\nSo, how are the best of the best pick its stocks? It is from the fundamental, the technical side or the sentimental side? Within this paper, we hope to bring another perspective, using machine learning models to predict the profitability of the stock price.\nPaper Outline\nFirst, we will list and explain the definition of each variable in our dataset. There are 25 variables in total as listed below. The variable we are going to predict is PROFIT. Then, we will start to process our data by merging with the macro data and removing some unwanted variables. After processing the data, we will have a data visualization section that shows the distribution and relationship between some variables. And to find the best model, we choose to explore three machine learning models which are LASSO, Random forest and Stacking models. We use a stacking method to create the stacking model by combining three models (Lasso, Random Forest, and KNN). Lastly, we will make a stock return prediction for 2021 using the trained model with the lowest R-squared and RMSE.\nList of Variables\nFor the dataset, we includes financial information on companies in the S&P 500 stock index from 1999-2021. This information was scraped from Yahoo Finance in November of 2021, and collected in a csv format for data analysis. The information includes metrics like sales, earnings, cogs, stock price, and market sector as well as macroeconomic data such as GDP or Money Supply. The goal is to analyze and model this data to better improve projections for a company’s future profitability. The variables in the data set are described below:\nVariable\nMeaning\nYEAR\nThe financial year of the company\nCOMPANY\nThe company’s stock abbreviation symbol\nMARKET.CAP\nThe total market capitalization of the company (Volume * Price)\nEARNINGS\nThe earnings in dollars for the previous year for the given company\nSALES\nHow much the company sold in dollars last year\nCASH\nHow much cash the company has in dollars at the end of the previous year\nName\nThe full name of the company\nSector\nThe name of the sector that the company is a part of\nEarnings_next_year\nThe amount of money in dollars that the company earns in the following year\nPRICE\nThe price of the stock when it is bought\nSell\nThe price of the stock when it is sold\nVOLUME\nThe total number of shares that the company holds\nCOGS\nThe total amount the company paid as a cost directly related to the sale of products\nINVESTMENT\nThe total asset or item acquired with the goal of generating income or appreciation\nRECIEVABLE\nThe debts owed to a company by its customers for goods that have been delivered or used but not yet paid for\nINVENTORY\nHow much raw materials used in production as well as the goods produced that are available for sale\nDEBTS\nHow much money the company borrow from other parties\nCPALTT01USM657N_PC1\nThe percentage change in CPI (measure of inflation)\nGDP\nThe monetary value of all finished goods and services made within a country\nGDP_PC1\nThe percentage change in GDP\nT10Y2Y\nTen year treasury bonds minus two year treasury bonds\nM1SL\nThe total currency and other liquid instruments in a country’s economy\nM1SL_PC1\nThe percentage change in money supply\nEarnings_next_year\nHow much profit that a company produces next year\nPROFIT\nHow much the money made or lost on an investment\nLoading data\n\nhide\nfinalDATASET <- read_csv(\"FINALDATASET.csv\")\nmacro_data <- read_csv(\"clean_macro - Sheet1.csv\")\n\n\n\nData Preprocessing\nFor the data preprocessing, we combined our data sources and filled missing values for some fundamental factors by using the median of that value in the sector for a certain year. We also created sector dummies in this step. We then dropped some variables that are not in our interests and the splited the testing and trainning data.\n\nhide\nfinal_data <- finalDATASET %>% \n  # make Sector dummy variables\n  dummy_cols(select_columns = \"Sector\") %>% \n  # merge with updated macro data\n  select(-CPALTT01USM657N_PC1,\n         -GDP,\n         -GDP_PC1,\n         -M1SL_PC1 ,\n         -M1SL,\n         -PRICE,\n         -Sell,\n         -COMPANY) %>% \n  merge(macro_data) %>% \n  # convert Macro factors from characters to numeric\n  mutate(across(c(\"CPALTT01USM657N_PC1\",\"GDP\",\"GDP_PC1\",\"T10Y2Y\",\"M1SL_PC1\",\"M1SL\"),\n                as.numeric)) %>% \n  group_by(YEAR,Sector) %>% \n  # replacing the missing value with median od the industry in that year\n  mutate(across(c(DEBTS,INVESTMENTS,CASH,VOLUME,EARNINGS,COGS,SALES,RECEIVABLE,INVENTORY),~replace(.,.==0,median(.)))) %>%\n  # delete less important factors -> Exchange can possibily be deleted\n  ungroup() %>%  \n  mutate(across(c(!where(is.numeric),-\"Name\"),as.factor)) %>% \n  select(-Earnings_next_year,-observation_date) %>% \n  drop_na() \n\n# filter out the data for 2021\nfinal_data_2021 <- final_data %>% \n  filter(YEAR == 2021)\n\nfinal_data <- final_data %>% \n  filter(YEAR < 2021)\n\n# split the data\nset.seed(327) #for reproducibility\n\ndata_split <- initial_split(final_data, \n                             prop = .75)\ndata_training <- training(data_split)\ndata_testing <- testing(data_split)\n\n# quick look of the data \nfinal_data %>% \n  head(5) %>% \n  kbl() %>% \n  kable_styling(bootstrap_options = c(\"striped\", \"bordered\", \"hover\", \"condensed\")) %>% \n  scroll_box(width = \"100%\", height = \"300px\")\n\n\n\n\nYEAR\n\n\nT10Y2Y\n\n\nVOLUME\n\n\nMARKET CAP\n\n\nEARNINGS\n\n\nCOGS\n\n\nSALES\n\n\nCASH\n\n\nINVESTMENTS\n\n\nRECEIVABLE\n\n\nINVENTORY\n\n\nDEBTS\n\n\nName\n\n\nSector\n\n\nPROFIT\n\n\nSector_Communication Services\n\n\nSector_Consumer Discretionary\n\n\nSector_Consumer Staples\n\n\nSector_Energy\n\n\nSector_Financials\n\n\nSector_Health Care\n\n\nSector_Industrials\n\n\nSector_Information Technology\n\n\nSector_Materials\n\n\nSector_Real Estate\n\n\nSector_Utilities\n\n\nCPALTT01USM657N_PC1\n\n\nGDP\n\n\nGDP_PC1\n\n\nM1SL_PC1\n\n\nM1SL\n\n\n1999\n\n\n0.2\n\n\n6426000000\n\n\n176614593750\n\n\n1350072000\n\n\n2917617000\n\n\n8458777000\n\n\n534652000\n\n\n1156849000\n\n\n1297867000\n\n\n361986000\n\n\n1900000\n\n\nCisco Systems\n\n\nInformation Technology\n\n\n139.56789\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n0\n\n\n65.9\n\n\n9631.127\n\n\n6.3\n\n\n2\n\n\n1102.3\n\n\n1999\n\n\n0.2\n\n\n136000000\n\n\n745875000\n\n\n26900000\n\n\n813900000\n\n\n1458600000\n\n\n202900000\n\n\n0\n\n\n220900000\n\n\n31200000\n\n\n51400000\n\n\nQuest Diagnostics\n\n\nHealth Care\n\n\n63.53276\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n65.9\n\n\n9631.127\n\n\n6.3\n\n\n2\n\n\n1102.3\n\n\n1999\n\n\n0.2\n\n\n1603286905\n\n\n32386397084\n\n\n156200000\n\n\n860700000\n\n\n1331200000\n\n\n198600000\n\n\n0\n\n\n795500000\n\n\n0\n\n\n374196000\n\n\nInvesco\n\n\nFinancials\n\n\n15.35842\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n65.9\n\n\n9631.127\n\n\n6.3\n\n\n2\n\n\n1102.3\n\n\n1999\n\n\n0.2\n\n\n91032448\n\n\n1217558992\n\n\n102000000\n\n\n991300000\n\n\n1407900000\n\n\n95600000\n\n\n0\n\n\n230000000\n\n\n128300000\n\n\n157900000\n\n\nPerkinElmer\n\n\nHealth Care\n\n\n156.29907\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n65.9\n\n\n9631.127\n\n\n6.3\n\n\n2\n\n\n1102.3\n\n\n1999\n\n\n0.2\n\n\n73297297\n\n\n848645267\n\n\n56500000\n\n\n162900000\n\n\n522100000\n\n\n61500000\n\n\n0\n\n\n288300000\n\n\n0\n\n\n374196000\n\n\nArthur J. Gallagher & Co. \n\n\nFinancials\n\n\n11.09317\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n65.9\n\n\n9631.127\n\n\n6.3\n\n\n2\n\n\n1102.3\n\n\n\nData Visualization\nWe first explored distributions of our predictors and outcome. As the following graph suggested, all the numeric variables are severely right-skewed with some outliers. In our data, though the numbers of firms in each sector are unbalanced, we got a decent amount of data for each sector.\n\nhide\n  final_data %>% \n  select(where(is.numeric)) %>% \n  select(-starts_with(\"Sector_\")) %>% \n  pivot_longer(cols = everything(),\n               names_to = \"variable\", \n               values_to = \"value\") %>% \n  ggplot(aes(x = value)) +\n  geom_histogram(bins = 30) +\n  facet_wrap(vars(variable), \n             scales = \"free\",\n             nrow = 4,\n             ncol=5)\n\n\n\n\n\nhide\nfinal_data %>% \n  select(where(is.factor)) %>% \n  pivot_longer(cols = everything(),\n               names_to = \"variable\", \n               values_to = \"value\") %>% \n  ggplot(aes(x = value)) +\n  geom_bar() +\n  facet_wrap(vars(variable), \n             scales = \"free\", \n             nrow = 4) + \n  theme(axis.text.x = element_text(angle = 45))\n\n\n\n\nThen, we continued to explore the relationship between regressors and the outcome. By observation, we can’t see a strong correlation between fundemental factors, such as investments and debts, and stock return. It may indicate that linear model is not an ideal model in this case.\n\nhide\nfinal_data %>% \n  ggplot(aes(x = INVESTMENTS, y = PROFIT,color = YEAR)) + \n  geom_point(alpha = 0.5)+\n  geom_smooth(se = FALSE) + \n  labs(title = \"Relationship between investments and stock return\")\n\n\n\n\n\nhide\nfinal_data %>% \n  ggplot(aes(x = DEBTS, y = PROFIT,color = YEAR)) + \n  geom_point(alpha = 0.5)+\n  geom_smooth(se = FALSE) + \n  labs(title = \"Relationship between debts and stock return\")\n\n\n\n\nWith this anamiation, we could see that the returns of stocks follow a cycle, which may be influenced by macroeconomic condition. The return in 1999 and 2019 seems to be the highest for all industries. Also, we noticed that some industries vary a lot year to year, such as IT and healthcare industry.\n\nhide\nsector_return_an<-final_data %>%\n  ggplot(aes(x = PROFIT, y = Sector)) +\n  geom_boxplot(aes(color = Sector),\n             alpha = .8,\n             size = 1) + \n  labs(title = \"Spread of stock return in different sector\",\n       subtitle = \"YEAR: {closest_state}\",\n       color = \"\") + \n  transition_states(YEAR)\n  \nanimate(sector_return_an, duration = 25)\nanim_save(\"sector_return.gif\")\n\n\n\n\nhide\nknitr::include_graphics(\"sector_return.gif\")\n\n\n\n\nWe further explored the relationship between the industry and the potential macroeconomic influencer using graphs below. Though the relationship doesn’t seem to be linear, we do see how macroeconomic condition affect each industry differently and will account for that in our model.\n\nhide\nfinal_data %>% \n  group_by(YEAR,Sector) %>% \n  mutate(profit_por = PROFIT*`MARKET CAP`/(sum(`MARKET CAP`,na.rm = TRUE))) %>% \n  summarise(Sector_profit = sum(profit_por),\n            GDP_PC1 = mean(GDP_PC1),\n            CPALTT01USM657N_PC1 = mean(CPALTT01USM657N_PC1),\n            Sector = Sector[1]) %>% \n  ggplot(aes(x = CPALTT01USM657N_PC1,y = Sector_profit,color = Sector))+\n  geom_smooth(se = FALSE) + \n  labs(title = \"Relationship between inflation and average return of a industry\",\n       x = \"Percent Change in CPI\",\n       y = \"Average Return of a sector\")\n\n\n\n\n\nhide\nfinal_data %>% \n  group_by(YEAR,Sector) %>% \n  mutate(profit_por = PROFIT*`MARKET CAP`/(sum(`MARKET CAP`,na.rm = TRUE))) %>% \n  summarise(Sector_profit = sum(profit_por),\n            GDP_PC1 = mean(GDP_PC1),\n            CPALTT01USM657N_PC1 = mean(CPALTT01USM657N_PC1),\n            Sector = Sector[1]) %>% \n  ggplot(aes(x = GDP_PC1,y = Sector_profit,color = Sector))+\n  geom_smooth(se = FALSE) + \n  labs(title = \"Relationship between GDP  and average return of a industry\",\n       x = \"Percent Change in GDP\",\n       y = \"Average Return of a sector\")\n\n\n\n\nLasso Model\nBuilding Recipe\nIn the lasso model, to account for the fact the macroeconomic condition affects each industry differently, we also included the interaction term between GDP and sector dummies.\n\nhide\nreturn_recipe <- recipe(PROFIT ~ ., #short-cut, . = all other vars\n                       data = data_training) %>% \n  # filter to only have data after 2020\n  step_filter(YEAR<2021) %>% \n  step_rm(Name,Sector,YEAR) %>% \n  #step_rm(GDP,M1SL,Name,Sector,YEAR) %>% \n  # add PE \n  step_mutate(PE = `MARKET CAP`/EARNINGS) %>%\n  # Normalize all variables except for GDP\n  step_normalize(all_predictors(), \n                 -all_nominal(),\n                 -starts_with(\"Sector_\")) %>% \n  # Create interaction terms\n  step_interact(terms = ~c(GDP_PC1):starts_with(\"Sector_\")) \n# show the data in recipe\nreturn_recipe %>% \n  prep(data_training) %>%\n  # using bake(new_data = NULL) gives same result as juice()\n  # bake(new_data = NULL)\n  juice() %>% \n  head(10) %>% \n  kbl() %>% \n  kable_styling(bootstrap_options = c(\"striped\", \"bordered\", \"hover\", \"condensed\")) %>% \n  scroll_box(width = \"100%\", height = \"400px\")\n\n\n\n\nT10Y2Y\n\n\nVOLUME\n\n\nMARKET CAP\n\n\nEARNINGS\n\n\nCOGS\n\n\nSALES\n\n\nCASH\n\n\nINVESTMENTS\n\n\nRECEIVABLE\n\n\nINVENTORY\n\n\nDEBTS\n\n\nSector_Communication Services\n\n\nSector_Consumer Discretionary\n\n\nSector_Consumer Staples\n\n\nSector_Energy\n\n\nSector_Financials\n\n\nSector_Health Care\n\n\nSector_Industrials\n\n\nSector_Information Technology\n\n\nSector_Materials\n\n\nSector_Real Estate\n\n\nSector_Utilities\n\n\nCPALTT01USM657N_PC1\n\n\nGDP\n\n\nGDP_PC1\n\n\nM1SL_PC1\n\n\nM1SL\n\n\nPROFIT\n\n\nPE\n\n\nGDP_PC1_x_Sector_Communication Services\n\n\nGDP_PC1_x_Sector_Consumer Discretionary\n\n\nGDP_PC1_x_Sector_Consumer Staples\n\n\nGDP_PC1_x_Sector_Energy\n\n\nGDP_PC1_x_Sector_Financials\n\n\nGDP_PC1_x_Sector_Health Care\n\n\nGDP_PC1_x_Sector_Industrials\n\n\nGDP_PC1_x_Sector_Information Technology\n\n\nGDP_PC1_x_Sector_Materials\n\n\nGDP_PC1_x_Sector_Real Estate\n\n\nGDP_PC1_x_Sector_Utilities\n\n\n-0.2723735\n\n\n-0.2776796\n\n\n-0.1486855\n\n\n-0.0917829\n\n\n0.0580188\n\n\n-0.0545930\n\n\n-0.0288145\n\n\n-0.1272508\n\n\n0.2467676\n\n\n-0.0229636\n\n\n-0.0859276\n\n\n1\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0.2262963\n\n\n0.8675619\n\n\n-0.4855693\n\n\n-0.2097115\n\n\n0.2272373\n\n\n4.862932\n\n\n-0.0267204\n\n\n-0.4855693\n\n\n0.000000\n\n\n0.000000\n\n\n0\n\n\n0.0000000\n\n\n0.0000000\n\n\n0.0000000\n\n\n0\n\n\n0.0000000\n\n\n0.0000000\n\n\n0\n\n\n1.0114941\n\n\n-0.3645483\n\n\n-0.3727731\n\n\n-0.3302514\n\n\n-0.3397521\n\n\n-0.4254379\n\n\n-0.1789608\n\n\n-0.1259446\n\n\n-0.2323726\n\n\n-0.3437728\n\n\n-0.1643141\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n-0.3631769\n\n\n0.5412816\n\n\n0.1718292\n\n\n-0.1511675\n\n\n0.0535694\n\n\n31.080973\n\n\n0.0427583\n\n\n0.0000000\n\n\n0.000000\n\n\n0.000000\n\n\n0\n\n\n0.0000000\n\n\n0.1718292\n\n\n0.0000000\n\n\n0\n\n\n0.0000000\n\n\n0.0000000\n\n\n0\n\n\n1.2449246\n\n\n0.0645478\n\n\n0.2414889\n\n\n0.1524756\n\n\n0.5823000\n\n\n0.4369685\n\n\n-0.0259228\n\n\n-0.1280716\n\n\n0.2363949\n\n\n0.8964895\n\n\n-0.0985802\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0.0123394\n\n\n-0.0149473\n\n\n-0.0473036\n\n\n-0.0502295\n\n\n-0.2689624\n\n\n3.833942\n\n\n-0.0092193\n\n\n0.0000000\n\n\n0.000000\n\n\n0.000000\n\n\n0\n\n\n0.0000000\n\n\n0.0000000\n\n\n-0.0473036\n\n\n0\n\n\n0.0000000\n\n\n0.0000000\n\n\n0\n\n\n0.8947789\n\n\n-0.3967183\n\n\n-0.3684586\n\n\n-0.1615117\n\n\n-0.3627474\n\n\n-0.3298581\n\n\n-0.1623081\n\n\n0.7461020\n\n\n-0.1097635\n\n\n-0.3870415\n\n\n-0.1823930\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n-0.2725085\n\n\n0.3395715\n\n\n-0.0911302\n\n\n-0.1572237\n\n\n-0.0528192\n\n\n14.945191\n\n\n-0.0785878\n\n\n0.0000000\n\n\n0.000000\n\n\n0.000000\n\n\n0\n\n\n-0.0911302\n\n\n0.0000000\n\n\n0.0000000\n\n\n0\n\n\n0.0000000\n\n\n0.0000000\n\n\n0\n\n\n1.2449246\n\n\n0.4939553\n\n\n-0.0346684\n\n\n0.1980059\n\n\n0.8122755\n\n\n0.8375368\n\n\n-0.1811591\n\n\n-0.1045226\n\n\n-0.2200460\n\n\n2.2750369\n\n\n-0.0843783\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n4.4982437\n\n\n-0.3347406\n\n\n-2.5454182\n\n\n-0.0744546\n\n\n-0.4180667\n\n\n31.682900\n\n\n-0.0558299\n\n\n0.0000000\n\n\n-2.545418\n\n\n0.000000\n\n\n0\n\n\n0.0000000\n\n\n0.0000000\n\n\n0.0000000\n\n\n0\n\n\n0.0000000\n\n\n0.0000000\n\n\n0\n\n\n-1.4395259\n\n\n-0.3311706\n\n\n-0.4382907\n\n\n-0.3489899\n\n\n-0.3069941\n\n\n-0.4045970\n\n\n-0.1836506\n\n\n-0.1280716\n\n\n-0.2095608\n\n\n-0.3170927\n\n\n-0.1855777\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n-0.3023032\n\n\n-0.5236186\n\n\n0.9607075\n\n\n-0.3570810\n\n\n-0.5234134\n\n\n20.995936\n\n\n-0.0307161\n\n\n0.0000000\n\n\n0.000000\n\n\n0.000000\n\n\n0\n\n\n0.0000000\n\n\n0.0000000\n\n\n0.0000000\n\n\n0\n\n\n0.9607075\n\n\n0.0000000\n\n\n0\n\n\n-1.6729564\n\n\n-0.2237800\n\n\n-0.4552338\n\n\n-0.3738869\n\n\n-0.3595441\n\n\n-0.4587357\n\n\n-0.1945849\n\n\n-0.1280716\n\n\n-0.2405498\n\n\n-0.2850657\n\n\n-0.1941957\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n-0.1707954\n\n\n-1.5399158\n\n\n1.1360138\n\n\n-0.3590997\n\n\n-0.6322063\n\n\n11.643812\n\n\n0.1030321\n\n\n0.0000000\n\n\n0.000000\n\n\n0.000000\n\n\n0\n\n\n0.0000000\n\n\n0.0000000\n\n\n1.1360138\n\n\n0\n\n\n0.0000000\n\n\n0.0000000\n\n\n0\n\n\n-1.2060955\n\n\n0.3081610\n\n\n-0.0327128\n\n\n-0.1563413\n\n\n-0.2160964\n\n\n-0.2207183\n\n\n-0.1852450\n\n\n-0.1280716\n\n\n-0.1694473\n\n\n-0.1451228\n\n\n-0.1450700\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n-0.0677982\n\n\n-1.7166315\n\n\n1.0921872\n\n\n-0.3207433\n\n\n-0.6327673\n\n\n15.293548\n\n\n0.0406114\n\n\n0.0000000\n\n\n0.000000\n\n\n1.092187\n\n\n0\n\n\n0.0000000\n\n\n0.0000000\n\n\n0.0000000\n\n\n0\n\n\n0.0000000\n\n\n0.0000000\n\n\n0\n\n\n-1.0893802\n\n\n-0.2873667\n\n\n-0.0954271\n\n\n-0.2312848\n\n\n-0.3613798\n\n\n-0.3727361\n\n\n-0.1369855\n\n\n-0.0416744\n\n\n-0.2173061\n\n\n-0.3870415\n\n\n-0.1942280\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\n-0.0852640\n\n\n-0.3358354\n\n\n0.4347887\n\n\n-0.3651560\n\n\n-0.5242549\n\n\n-21.545285\n\n\n0.0860387\n\n\n0.0000000\n\n\n0.000000\n\n\n0.000000\n\n\n0\n\n\n0.0000000\n\n\n0.0000000\n\n\n0.0000000\n\n\n0\n\n\n0.0000000\n\n\n0.4347887\n\n\n0\n\n\n-0.2723735\n\n\n-0.3487931\n\n\n-0.2821674\n\n\n-0.3298109\n\n\n-0.3547113\n\n\n-0.4414711\n\n\n-0.1920806\n\n\n-0.1280716\n\n\n-0.2337550\n\n\n-0.3870415\n\n\n-0.1903547\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\n0.2262963\n\n\n0.8675619\n\n\n-0.4855693\n\n\n-0.2097115\n\n\n0.2272373\n\n\n-12.137741\n\n\n0.1864751\n\n\n0.0000000\n\n\n0.000000\n\n\n0.000000\n\n\n0\n\n\n0.0000000\n\n\n0.0000000\n\n\n0.0000000\n\n\n0\n\n\n0.0000000\n\n\n-0.4855693\n\n\n0\n\n\n\nSelect tuning parameter\n\nhide\nreturn_linear_mod <- \n  # Define a lasso model \n  # I believe default is mixture = 1 so probably don't need \n  linear_reg(mixture = 1) %>% \n  # Set the engine to \"glmnet\" \n  set_engine(\"glmnet\") %>% \n  # The parameters we will tune.\n  set_args(penalty = tune()) %>% \n  # Use \"regression\"\n  set_mode(\"regression\")\n\nset.seed(456)\nreturn_lm_wf <- \n  # Set up the workflow\n  workflow() %>% \n  # Add the recipe\n  add_recipe(return_recipe) %>% \n  # Add the modeling\n  add_model(return_linear_mod)\n\npenalty_grid <- grid_regular(penalty(),\n                             levels = 10)\n\nreturn_cv <- vfold_cv(data_training, v = 5)\n\nreturn_lm_tune <- \n  return_lm_wf %>% \n  tune_grid(\n    resamples = return_cv,\n    grid = penalty_grid\n    )\n\nbest_param<-return_lm_tune %>% \n  select_best(metric = \"rmse\")\n\nreturn_lasso_final_wf <- return_lm_wf %>% \n  finalize_workflow(best_param)\n\nreturn_lasso_final_mod <- return_lasso_final_wf %>% \n  fit(data = data_training)\n\n# visulization for best param\nset.seed(456)\nreturn_lm_tune %>% \n  collect_metrics() %>% \n  filter(.metric == \"rmse\") %>% \n  ggplot(aes(x = penalty, y = mean)) +\n  geom_point() +\n  geom_line() +\n  scale_x_log10(\n   breaks = scales::trans_breaks(\"log10\", function(x) 10^x),\n   labels = scales::trans_format(\"log10\",scales::math_format(10^.x))) +\n  labs(x = \"penalty\", y = \"rmse\")\n\n\n\n\nLasso results\nThe table below shows the estimate of each predictor in the Lasso Model. We can see that indicators in macroeconomics are relatively important to predict the stock return. The change in inflation and GDP all remain significant after shrinking. The interaction terms between GDP and industries also showed importance, accounting for the fact that the macroeconomic condition affects each industry differently: GDP seems to affect the stock return in Communication Services, Energy, and Consumer Discretionary sectors less.\n\nhide\nreturn_lasso_final_mod %>% \n  pull_workflow_fit() %>% \n  tidy() %>% \n  arrange(desc(estimate)) %>% \n  kbl() %>% \n  kable_styling(bootstrap_options = c(\"striped\", \"bordered\", \"hover\", \"condensed\")) %>% \n  scroll_box(width = \"100%\", height = \"500px\")\n\n\n\n\nterm\n\n\nestimate\n\n\npenalty\n\n\nM1SL\n\n\n52.2489802\n\n\n0.0059948\n\n\nCPALTT01USM657N_PC1\n\n\n22.8362276\n\n\n0.0059948\n\n\nGDP_PC1\n\n\n18.5139718\n\n\n0.0059948\n\n\n(Intercept)\n\n\n17.4206737\n\n\n0.0059948\n\n\nGDP_PC1_x_Sector_Utilities\n\n\n14.5169194\n\n\n0.0059948\n\n\nGDP_PC1_x_Sector_Consumer Staples\n\n\n10.9623359\n\n\n0.0059948\n\n\nGDP_PC1_x_Sector_Health Care\n\n\n10.0994574\n\n\n0.0059948\n\n\nSector_Information Technology\n\n\n9.1062940\n\n\n0.0059948\n\n\nT10Y2Y\n\n\n8.3785234\n\n\n0.0059948\n\n\nGDP_PC1_x_Sector_Real Estate\n\n\n7.9131832\n\n\n0.0059948\n\n\nSector_Communication Services\n\n\n7.4629557\n\n\n0.0059948\n\n\nSector_Health Care\n\n\n5.9942110\n\n\n0.0059948\n\n\nSector_Consumer Discretionary\n\n\n4.8412271\n\n\n0.0059948\n\n\nGDP_PC1_x_Sector_Information Technology\n\n\n4.0219608\n\n\n0.0059948\n\n\nSector_Energy\n\n\n2.7587880\n\n\n0.0059948\n\n\nCOGS\n\n\n2.2012752\n\n\n0.0059948\n\n\nGDP_PC1_x_Sector_Industrials\n\n\n2.0052201\n\n\n0.0059948\n\n\nVOLUME\n\n\n0.9051955\n\n\n0.0059948\n\n\nINVESTMENTS\n\n\n0.4845378\n\n\n0.0059948\n\n\nCASH\n\n\n0.4031170\n\n\n0.0059948\n\n\nEARNINGS\n\n\n0.3671604\n\n\n0.0059948\n\n\nRECEIVABLE\n\n\n0.1688682\n\n\n0.0059948\n\n\nINVENTORY\n\n\n0.1677270\n\n\n0.0059948\n\n\nGDP_PC1_x_Sector_Financials\n\n\n0.0413482\n\n\n0.0059948\n\n\nSector_Materials\n\n\n0.0000000\n\n\n0.0059948\n\n\nPE\n\n\n-0.2304146\n\n\n0.0059948\n\n\nDEBTS\n\n\n-0.2444685\n\n\n0.0059948\n\n\nSector_Industrials\n\n\n-0.4250661\n\n\n0.0059948\n\n\nGDP_PC1_x_Sector_Materials\n\n\n-1.0239217\n\n\n0.0059948\n\n\nGDP_PC1_x_Sector_Communication Services\n\n\n-1.4192763\n\n\n0.0059948\n\n\nSector_Financials\n\n\n-1.5534082\n\n\n0.0059948\n\n\nSALES\n\n\n-3.2886701\n\n\n0.0059948\n\n\nSector_Real Estate\n\n\n-3.3504621\n\n\n0.0059948\n\n\nMARKET CAP\n\n\n-3.8678288\n\n\n0.0059948\n\n\nSector_Consumer Staples\n\n\n-5.4154183\n\n\n0.0059948\n\n\nGDP_PC1_x_Sector_Energy\n\n\n-5.8821361\n\n\n0.0059948\n\n\nGDP_PC1_x_Sector_Consumer Discretionary\n\n\n-8.6297519\n\n\n0.0059948\n\n\nSector_Utilities\n\n\n-8.7405789\n\n\n0.0059948\n\n\nM1SL_PC1\n\n\n-11.4973966\n\n\n0.0059948\n\n\nGDP\n\n\n-16.0930330\n\n\n0.0059948\n\n\n\nModel Evaluation\nPrediciton precision\nThe Lasso model does not perform well. It only explains 20% of the varaince in stock return and it also has a relatively high rmse of around 46.\n\nhide\nreturn_lm_tune %>%\n  select(id, .metrics) %>%\n  unnest(.metrics) %>%\n  filter(.metric == \"rsq\") %>% \n  filter(.config == \"Preprocessor1_Model08\") %>% \n  summarise(mean_rsq = mean(.estimate)) \n\n\n# A tibble: 1 x 1\n  mean_rsq\n     <dbl>\n1    0.208\nhide\nreturn_lm_tune %>%\n  select(id, .metrics) %>%\n  unnest(.metrics) %>%\n  filter(.metric == \"rmse\") %>% \n  filter(.config == \"Preprocessor1_Model08\") %>% \n  summarise(mean_rmse = mean(.estimate)) \n\n\n# A tibble: 1 x 1\n  mean_rmse\n      <dbl>\n1      45.5\n\n\nhide\nset.seed(456)\nprediction <- predict(\n  return_lasso_final_mod,\n  new_data = data_training)\n\ntraining_pred<-data_training %>% \n  mutate(.pred = prediction$.pred)\n  \n  \ntraining_pred %>% \n  ggplot(aes(x = PROFIT, \n             y = .pred,\n             color = YEAR)) +\n  geom_point(alpha = .5, \n             size = .5) +\n  geom_smooth(se = FALSE) +\n  geom_abline(slope = 1, \n              intercept = 0, \n              color = \"darkred\") +\n  geom_text(aes(label = Name),label.size = 0.15,data = training_pred %>% filter(PROFIT>400) )+\n  labs(x = \"Actual Return\", \n       y = \"Predicted Return\") +\n  scale_color_viridis_b()\n\n\n\nOverfitting\nBelow shows the r-square and rmse on testing data. They are very similar to those on trainning data so we would safely conclude that our model did not overfit.\n\nhide\nreturn_lasso_test <- return_lasso_final_wf %>% \n  last_fit(data_split)\n\n# Metrics for model applied to test data\nreturn_lasso_test %>% \n  collect_metrics() \n\n\n# A tibble: 2 x 4\n  .metric .estimator .estimate .config             \n  <chr>   <chr>          <dbl> <chr>               \n1 rmse    standard      52.6   Preprocessor1_Model1\n2 rsq     standard       0.206 Preprocessor1_Model1\n\nThe following graphs show the performance of the model on the testing data:\n\nhide\ntest_prediction <- predict(\n  return_lasso_final_mod,\n  new_data = data_testing)\n\ntesting_lasso_pred<-data_testing %>% \n  mutate(.pred = test_prediction$.pred)\n  \n  \ntesting_lasso_pred %>% \n  ggplot(aes(x = PROFIT, \n             y = .pred,\n             color = YEAR)) +\n  geom_point(alpha = .5, \n             size = .5) +\n  geom_smooth(se = FALSE) +\n  geom_abline(slope = 1, \n              intercept = 0, \n              color = \"darkred\") +\n  geom_text(aes(label = Name),label.size = 0.15,data = testing_lasso_pred %>% filter(PROFIT>300) )+\n  labs(x = \"Actual Return\", \n       y = \"Predicted Return\") + \n  scale_color_viridis_b()\n\n\n\n\nInterpretable Machine Learning\nAgain, the boxplot and the histogram of residuals, the model does not predict the return well and this may because some extreme values.\n\nhide\nlasso_explain <- \n  explain_tidymodels(\n    model = return_lasso_final_mod,\n    data = data_training %>% select(-PROFIT), \n    y = data_training %>%  pull(PROFIT),\n    label = \"lasso\"\n  )\n\n\nPreparation of a new explainer is initiated\n  -> model label       :  lasso \n  -> data              :  7172  rows  30  cols \n  -> data              :  tibble converted into a data.frame \n  -> target variable   :  7172  values \n  -> predict function  :  yhat.workflow  will be used ( [33m default [39m )\n  -> predicted values  :  No value for predict function target column. ( [33m default [39m )\n  -> model_info        :  package tidymodels , ver. 0.1.3 , task regression ( [33m default [39m ) \n  -> predicted values  :  numerical, min =  -49.61277 , mean =  19.22262 , max =  121.2461  \n  -> residual function :  difference between y and yhat ( [33m default [39m )\n  -> residuals         :  numerical, min =  -146.2719 , mean =  -4.401566e-14 , max =  1002.161  \n [32m A new explainer has been created! [39m \n\n\nhide\nlasso_mod_perf <- model_performance(lasso_explain)\nhist_plot <- \n  plot(lasso_mod_perf,\n       geom = \"histogram\")\nbox_plot <-\n  plot(lasso_mod_perf,\n       geom = \"boxplot\")\n\nhist_plot\n\n\nhide\nbox_plot\n\n\n\n\nBelow shows the feature importance plots generated by two different methods. We could see that macroeconomic indicators remained importance in both of the plots but market cap seems to be an crucial factor when we use permutation method.\n\nhide\nset.seed(10) #since we are sampling & permuting, we set a seed so we can replicate the results\nlasso_var_imp <- \n  model_parts(\n    lasso_explain\n    )\n\nplot(lasso_var_imp, show_boxplots = TRUE)\n\n\n\n\n\nhide\nreturn_lasso_final_mod %>% \n  pull_workflow_fit() %>% \n  vip()\n\n\n\n\nRandom Forest Model\nBuilding Random Forest model\nTo build the random forest model, we set up recipe using our training data (data_training), define model with mtry = 6, min_n = 10 and numbers of tree = 200, create ranger workflow, and then we can fit the model.\n\nhide\n# set up recipe \nranger_recipe <-\n  recipe(PROFIT ~ ., #short-cut, . = all other vars\n                       data = data_training) %>%\n  step_filter(YEAR<2021) %>% \n  # remove the unwanted variables\n  step_rm(YEAR,Name,GDP,M1SL,Sector) %>% \n  # add PE \n  step_mutate(PE = `MARKET CAP`/EARNINGS)\n\n#define model\nranger_spec <- \n  rand_forest(mtry = 6, \n              min_n = 10, \n              trees = 200) %>% \n  set_mode(\"regression\") %>% \n  set_engine(\"ranger\")\n\n#create workflow\nranger_workflow <- \n  workflow() %>% \n  add_recipe(ranger_recipe) %>% \n  add_model(ranger_spec)\n\n#fit the model\nset.seed(712) # for reproducibility - random sampling in random forest choosing number of variables\nranger_fit <- ranger_workflow %>% \n  fit(data_training)\n\n\n\nRandom Forest results\nThe table below shows the OOB error (MSE), OOB RMSE (Root mean square error), and R squared of the stock return predictions using the random forest model above.\n\nhide\nmetrics = c(\"OOB error\", \"OOB RMSE\",\"R Squared\")\ndata_frame(metrics = metrics, value=c(ranger_fit$fit$fit$fit$prediction.error,\n                            sqrt(ranger_fit$fit$fit$fit$prediction.error),\n                            ranger_fit$fit$fit$fit$r.squared))\n\n\n    metrics        value\n1 OOB error 1545.1346996\n2  OOB RMSE   39.3082014\n3 R Squared    0.4086307\n\nModel Evaluation\nPrediciton precision (Traning data)\nEven though the mean rmse from the random forest (39.30787) seems to be fairly high but it is still lower than the LASSO model of 46. Thus, random forest model performs better.\n\nhide\nset.seed(1211) # for reproducibility\ndata_cv <- vfold_cv(data_training, v = 5)\n\nmetric <- metric_set(rmse)\nctrl_res <- control_stack_resamples()\n\nranger_cv <- ranger_workflow %>% \n  fit_resamples(data_cv, \n                metrics = metric,\n                control = ctrl_res)\n\n# Evaluation metrics averaged over all folds:\ncollect_metrics(ranger_cv) \n\n\n# A tibble: 1 x 6\n  .metric .estimator  mean     n std_err .config             \n  <chr>   <chr>      <dbl> <int>   <dbl> <chr>               \n1 rmse    standard    39.3     5    2.45 Preprocessor1_Model1\n\nWe also plot a graph showing the actual return vs predicted return.\n\nhide\nranger_prediction <- predict(\n  ranger_fit,\n  new_data = data_training)\n\nranger_training_pred<-data_training %>% \n  mutate(.pred = ranger_prediction$.pred)\n  \n  \nranger_training_pred %>%\n  ggplot(aes(x = PROFIT,\n             y = .pred)) +\n  geom_point(alpha = .5,\n             size = .5) +\n  geom_smooth(se = FALSE) +\n  geom_abline(slope = 1,\n              intercept = 0,\n              color = \"darkred\") +\n  labs(x = \"Actual Return\",\n       y = \"Predicted Return\")\n\n\n\n\nPrediciton precision (Testing data)\nThe table below shows the rmse on testing data. It is a bit higher but fairly similar to the error on training data so we could say that our model did not overfit.\n\nhide\nset.seed(1211) # for reproducibility\ndata_cv <- vfold_cv(data_testing, v = 5)\n\nmetric <- metric_set(rmse)\nctrl_res <- control_stack_resamples()\n\nranger_cv <- ranger_workflow %>% \n  fit_resamples(data_cv, \n                metrics = metric,\n                control = ctrl_res)\n\n# Evaluation metrics averaged over all folds:\ncollect_metrics(ranger_cv) \n\n\n# A tibble: 1 x 6\n  .metric .estimator  mean     n std_err .config             \n  <chr>   <chr>      <dbl> <int>   <dbl> <chr>               \n1 rmse    standard    47.4     5    4.34 Preprocessor1_Model1\n\nBelow is the graph showing the actual return vs. predicted return on testing data.\n\nhide\nranger_test_prediction <- predict(\n  ranger_fit,\n  new_data = data_testing)\n\nranger_test_pred<-data_testing %>% \n  mutate(.pred = ranger_test_prediction$.pred)\n  \n  \nranger_test_pred %>%\n  ggplot(aes(x = PROFIT,\n             y = .pred)) +\n  geom_point(alpha = .5,\n             size = .5) +\n  geom_smooth(se = FALSE) +\n  geom_abline(slope = 1,\n              intercept = 0,\n              color = \"darkred\") +\n  labs(x = \"Actual Return\",\n       y = \"Predicted Return\")\n\n\n\n\nInterpretable Machine Learning\nBased on the box-plot and the histogram below, the residuals mostly lie between -50 to 50. But there are some a few outliners that can go up to 400.\n\nhide\nrf_explain <- \n  explain_tidymodels(\n    model = ranger_fit,\n    data = data_training %>% select(-PROFIT), \n    y = data_training %>%  pull(PROFIT),\n    label = \"rf\"\n  )\n\n\nPreparation of a new explainer is initiated\n  -> model label       :  rf \n  -> data              :  7172  rows  30  cols \n  -> data              :  tibble converted into a data.frame \n  -> target variable   :  7172  values \n  -> predict function  :  yhat.workflow  will be used ( [33m default [39m )\n  -> predicted values  :  No value for predict function target column. ( [33m default [39m )\n  -> model_info        :  package tidymodels , ver. 0.1.3 , task regression ( [33m default [39m ) \n  -> predicted values  :  numerical, min =  -76.57776 , mean =  19.57231 , max =  438.0276  \n  -> residual function :  difference between y and yhat ( [33m default [39m )\n  -> residuals         :  numerical, min =  -101.4695 , mean =  -0.3496883 , max =  599.5611  \n [32m A new explainer has been created! [39m \nhide\nrf_mod_perf <-  model_performance(rf_explain)\n\nhist_plot <- \n  plot(rf_mod_perf, \n       geom = \"histogram\")\nbox_plot <-\n  plot(rf_mod_perf, \n       geom = \"boxplot\")\n\nhist_plot\n\n\nhide\nbox_plot\n\n\n\n\nAccording to the feature importance bar chart below, we can see that the top three important features are market cap, earnings, M1SL_PC1, and GDP_PC1.\n\nhide\nset.seed(10) #since we are sampling & permuting, we set a seed so we can replicate the results\nrf_var_imp <- \n  model_parts(\n    rf_explain\n    )\nplot(rf_var_imp, show_boxplots = TRUE)\n\n\n\n\nAfter creating two models, we then move on to the final method: stacking. For the stacking model, we also add one more model: KNN to improve the accuracy rate of the model.\nStacking Model\nRandom Forest Candidate\n\nhide\nranger_recipe <- \n  recipe(formula = PROFIT ~ ., \n         data = data_training) %>% \n  # Make these evaluative variables, not included in modeling\n  update_role(all_of(c(\"YEAR\", \"Name\", \"Sector\")),\n              new_role = \"evaluative\")\n\nranger_spec <- \n  rand_forest(mtry = 6, \n              min_n = 10, \n              trees = 200) %>% \n  set_mode(\"regression\") %>% \n  set_engine(\"ranger\")\n\nranger_workflow <- \n  workflow() %>% \n  add_recipe(ranger_recipe) %>% \n  add_model(ranger_spec) \n\nranger_fit <- ranger_workflow %>% \n  fit(data_training)\n\nset.seed(1211) # for reproducibility\nfinal_data_cv <- vfold_cv(data_training, v = 5)\n\nmetric <- metric_set(rmse)\nctrl_res <- control_stack_resamples()\n\nranger_cv <- ranger_workflow %>% \n  fit_resamples(final_data_cv, \n                metrics = metric,\n                control = ctrl_res)\n\n\n\nLASSO Candidate\n\nhide\n# lasso recipe and transformation steps\nlasso_final_data_recipe <- recipe(PROFIT ~ ., \n                       data = data_training) %>% \n  #step_rm(Name, Sector, YEAR, COMPANY) %>%\n  update_role(all_of(c(\"Name\",\n                       \"Sector\",\n                       \"YEAR\")),\n              new_role = \"evaluative\") %>% \n  step_dummy(all_nominal(), \n             -all_outcomes(), \n             -has_role(match = \"evaluative\")) %>% \n  step_normalize(all_predictors(), \n                 -all_nominal())\n\n#define lasso model\nlasso_mod <- \n  linear_reg(mixture = 1) %>% \n  set_engine(\"glmnet\") %>% \n  set_args(penalty = tune()) %>% \n  set_mode(\"regression\")\n\n# create workflow\nlasso_wf <- \n  workflow() %>% \n  add_recipe(lasso_final_data_recipe) %>% \n  add_model(lasso_mod)\n\n# penalty grid - changed to 10 levels\npenalty_grid <- grid_regular(penalty(),\n                             levels = 10)\n\n# add ctrl_grid - assures predictions and workflows are saved\nctrl_grid <- control_stack_grid()\n\n# tune the model using the same cv samples as random forest\n\nlasso_tune <- \n  lasso_wf %>% \n  tune_grid(\n    resamples = final_data_cv,\n    grid = penalty_grid,\n    metrics = metric,\n    control = ctrl_grid\n    )\n\n\n\nKNN Candidate\n\nhide\n# create a model definition\nknn_mod <-\n  nearest_neighbor(\n    neighbors = tune(\"k\")\n  ) %>%\n  set_engine(\"kknn\") %>% \n  set_mode(\"regression\")\n\n# create the workflow\nknn_wf <- \n  workflow() %>% \n  add_model(knn_mod) %>%\n  add_recipe(lasso_final_data_recipe)\n\n# tune it using 4 tuning parameters\nknn_tune <- \n  knn_wf %>% \n  tune_grid(\n    final_data_cv,\n    metrics = metric,\n    grid = 4,\n    control = ctrl_grid\n  )\n\n\n\nStacking all candidates\nBy showing and visualize the members in our stacking model, we could see that random forest model contribute the most.\n\nhide\nfinal_data_stack <- \n  stacks() %>% \n  add_candidates(ranger_cv) %>% \n  add_candidates(lasso_tune) %>% \n  add_candidates(knn_tune)\n\nfinal_data_blend <- \n  final_data_stack %>% \n  blend_predictions()\n\nfinal_data_blend \n\n\n# A tibble: 2 x 3\n  member        type             weight\n  <chr>         <chr>             <dbl>\n1 ranger_cv_1_1 rand_forest       0.835\n2 knn_tune_1_4  nearest_neighbor  0.254\n\n\nhide\nautoplot(final_data_blend)\n\n\n\n\n\nhide\nfinal_data_blend$metrics %>% \n  filter(.metric == \"rmse\") %>% \n  summarise(mean_rmse = mean(mean)) \n\n\n# A tibble: 1 x 1\n  mean_rmse\n      <dbl>\n1      39.7\n\n\nhide\nfinal_data_final_stack <- final_data_blend %>% \n  fit_members()\n\n\n\nTable below shows the top10 prediction when we applied the model to testing data. Here, we can see that when the model predicts company with high rate of return, the actual return is high as well.\n\nhide\nfinal_data_final_stack %>% \n  predict(new_data = data_testing) %>% \n  bind_cols(data_testing) %>% \n  select(Name, .pred, PROFIT) %>% \n  arrange(desc(.pred)) %>% \n  head(10)\n\n\n# A tibble: 10 x 3\n   Name                  .pred PROFIT\n   <chr>                 <dbl>  <dbl>\n 1 Penn National Gaming   229. 1145. \n 2 Devon Energy           221.  239. \n 3 Nvidia                 217.  203. \n 4 Caesars Entertainment  213.  705. \n 5 Intuit                 211.   41.8\n 6 Ansys                  205.   81.8\n 7 PVH                    195.  207. \n 8 Occidental Petroleum   192.  158. \n 9 Verisign               192.  571. \n10 Illumina               191.  192. \n\nComparison of the three models\nWith the three models, we then move on to see which model performs the best:\nLasso model\n\nhide\ndata_frame(return_lm_tune %>%\n  select(id, .metrics) %>%\n  unnest(.metrics) %>%\n  filter(.metric == \"rsq\") %>% \n  filter(.config == \"Preprocessor1_Model08\") %>% \n  summarise(mean_rsq = mean(.estimate)),\n\nreturn_lm_tune %>%\n  select(id, .metrics) %>%\n  unnest(.metrics) %>%\n  filter(.metric == \"rmse\") %>% \n  filter(.config == \"Preprocessor1_Model08\") %>% \n  summarise(mean_rmse = mean(.estimate)))\n\n\n   mean_rsq mean_rmse\n1 0.2079744  45.48327\n\nRandom Forest Model\n\nhide\n#OOB RMSE\ndata.frame(mean_rmse = sqrt(ranger_fit$fit$fit$fit$prediction.error),\n# R squared\nmean_rsq = ranger_fit$fit$fit$fit$r.squared)\n\n\n  mean_rmse  mean_rsq\n1  38.73345 0.4257979\n\nStacking Model\n\nhide\n# Stacking model: \ndata.frame(final_data_blend$metrics %>% \n  filter(.metric == \"rmse\") %>% \n  summarise(mean_rmse = mean(mean)),\n\nfinal_data_blend$metrics %>% \n  filter(.metric == \"rsq\") %>% \n  summarise(mean_rsq = mean(mean)))\n\n\n  mean_rmse  mean_rsq\n1  39.73178 0.4162962\n\nHere, we can see that compared to the three models, even though Random Forest performs better than the stacking model, stacking model use features in the Random Forest along with additional features from KNN and lasso. With that reason, we will choose stacking model as our model choice.\nStock Return Prediction for 2021\nAfter picking our model, we then move on to use the model to predict the potential profit for 2021:\n\nhide\nset.seed(456)\nytd = c(80.30, 39.77, -8.26, 11.29, 22.08, 132.51, 137.04, -4.22, 39.77, 20.76, 26.41, 26.34, 41.50, 178.43, 18.70, 87.29, -6.16, 52.58, 8.70, -13.70)\nset.seed(456)\npred_2021<-final_data_final_stack %>% \n  predict(new_data = final_data_2021) %>% \n  bind_cols(final_data_2021) %>% \n  select(Name, .pred) %>% \n  arrange(desc(.pred)) %>% \n  head(20) %>% \n  mutate(actual_ytd = ytd) \npred_2021 %>% \n  kbl() %>% \n  kable_styling(bootstrap_options = c(\"striped\", \"bordered\", \"hover\", \"condensed\")) %>% \n  column_spec(c(1,3), color = ifelse(pred_2021$actual_ytd < 0, \"red\", \"darkgreen\")) %>% \n  scroll_box(width = \"100%\", height = \"500px\") \n\n\n\n\nName\n\n\n.pred\n\n\nactual_ytd\n\n\nAPA Corporation\n\n\n67.67959\n\n\n80.30\n\n\nMarathon Oil\n\n\n63.13998\n\n\n39.77\n\n\nNews Corp (Class B)\n\n\n63.06597\n\n\n-8.26\n\n\nUnder Armour (Class C)\n\n\n62.82090\n\n\n11.29\n\n\nDuPont\n\n\n62.70736\n\n\n22.08\n\n\nFord\n\n\n62.55623\n\n\n132.51\n\n\nNorwegian Cruise Line Holdings\n\n\n62.50470\n\n\n137.04\n\n\nUnder Armour (Class A)\n\n\n61.68342\n\n\n-4.22\n\n\nCarnival Corporation\n\n\n61.52755\n\n\n39.77\n\n\nDXC Technology\n\n\n59.89012\n\n\n20.76\n\n\nHalliburton\n\n\n59.45987\n\n\n26.41\n\n\nRaytheon Technologies\n\n\n58.35294\n\n\n26.34\n\n\nDevon Energy\n\n\n57.31403\n\n\n41.50\n\n\nSchlumberger\n\n\n57.14758\n\n\n178.43\n\n\nConocoPhillips\n\n\n56.75287\n\n\n18.70\n\n\nBaker Hughes\n\n\n56.39542\n\n\n87.29\n\n\nHess Corporation\n\n\n56.39404\n\n\n-6.16\n\n\nSouthwest Airlines\n\n\n56.14967\n\n\n52.58\n\n\nBristol Myers Squibb\n\n\n56.04499\n\n\n8.70\n\n\nBoeing\n\n\n55.79926\n\n\n-13.70\n\n\n\nConclusion\nThe weakness of our model is that the average error of prediction is still very high so if one wants to predict the exact return, our model won’t be ideal. But the strength of our model is that for stocks with extremely high returns, even though the prediction might not be that precise, highly likely, our model will predict positive returns. That means, in real life, if we choose the top stocks to invest in based on our prediction, it is less likely we are going to lose money. Another strength of using a model to help with investment is that it excludes our subjective feelings.\nTo make the model better, we could do more research and add more regressors. For example, some financial indicators that are important for value investing are not reflected in our model due to the lack of data. Examples of those variables include Price to Sales, Price to Cash Flow, and Price to Book. Also, since we are concerned about the long-term return and all fundamental factors usually take longer to affect the firms, it’s probably helpful to do return in 2 years or 3 years or include lag of some variables in our model.\n\n\n\n",
    "preview": "projects/2021-12-20-stockreturn/stockreturn_files/figure-html5/unnamed-chunk-4-1.png",
    "last_modified": "2021-12-21T09:32:54-06:00",
    "input_file": {}
  },
  {
    "path": "projects/ml_p2/",
    "title": "Machine Leanring Project 2: SVM Summary and Its Application on Penguin Species",
    "description": "In this project, I will give a brief overview of a new classification algorithm support vector machines (SVM) and use it to predict Penguin Species using palmerpenguins data in R.",
    "author": [
      {
        "name": "Rita Liu",
        "url": {}
      }
    ],
    "date": "2020-12-11",
    "categories": [],
    "contents": "\n\nContents\nSetting Up\nPart 1: SVM OverviewPlacement within the machine learning workflow\nDefinition\nTuning\nPotential drawbacks\n\nPart 2: Using SVM on Pengiuns Classification (Selected Variables)Kernel selection: linear, radial, or polynomial?\nComparision with the logistic model\n\nPart 3: Using SVM on Pengiuns Classification (All Variables)Kernel & tuning parameter selection\nComparison with logistic model\n\nPart 4: What the Model Shows?\nReference:\n\n\nSetting Up\n\nShow code\nlibrary(ggplot2)\nlibrary(caret)\nlibrary(e1071)\nlibrary(ROCR)\nlibrary(dplyr)\nlibrary(palmerpenguins)\nlibrary(pROC)\n\n\n\nPart 1: SVM Overview\nPlacement within the machine learning workflow\nSVM vs Logistic Regression: SMV is also a parametric model and it will be more advantageous when the boundaries between different catergories are not linear.\nSVM vs KNN: Both of the model allow non-linear boundaries for classification. However, when we have many predictors or features, SVM would perform better than KNN because KNN suffers from curse of dimensionality.\nSVM vs tree: I personally think that SVM is a less greedy algorithm comparing to tree model because tree makes best splits without considering the future split. However, for SVM, we won’t have such a concern.\nDefinition\nIn short, the SVM model uses a separating hyperplane to classify observations.\nA separating hyperplane \\(f(x_1,x_2,...,x_n)\\) is a plane in space which can separate different levels of the observations based on the features. The observations fall into two classes, we use -1 to represent one class and 1 to represents the other class,that is \\(y_{i} ∈ \\{-1,1\\}\\). As a result, a hyperplane has the following property: \\(y_{i}*f(x_1,x_2,...,x_n) > 0\\).\n\nHowever, there are many hyperplanes that can separate the observations, as we can see from the figure below.  Since we want to choose the plane that best separates two classes, we want the plane to have the farthest minimum distance or margin from the observations. In the language of math, that is, \\(y_{i}*f(x_1,x_2,...,x_n) > = M\\), where M is the farthest minimum distance from the hyperplane to the observations.\nAs we can see from the figure below, three training observations are on the dashed line, meaning that the distance between these three observations and the separating hyperplane is the margin. Those points are support vectors, because if they move, the hyperplane would change. \nNot all observations can be perfectly separated into two classes by a plane and we could allow misclassifying of some a few observations by adding slack variables \\(ε\\) and the hyperplane satisfies \\(y_{i}*f(x_1,x_2,...,x_n) > = M*(1-ε_i)\\). When \\(ε_i=0\\), the observation is on the correct side of the margin; when \\(ε_i>0\\), the observation is on the wrong side of the margin; and when \\(ε_i>1\\), it’s on the wrong side of the plane. For example, as the graph below demonstrates, observation 12 and 11 would have a slack variable bigger than 1 because they are on the wrong side of the hyperplane.\n\n\nHyperplane function \\(f(x_i)\\) can have different forms or take in different kernels which allow us to have non-linear boundaries. For example, in the graph below, the figure on the left uses a polynomial kernel and the figure on the right uses a radial kernel.\nTuning\nThe tuning parameter for SVM is cost, which is defined as the sum of \\(εi\\). When the cost is big, the width of the margin will be wilder and many points can violate the margin. While when the cost is small, the width of the margin will be small and fewer misclassifications will be tolerated.\nBesides cost C, there are other tuning parameters according to what kernels we use. If we use the radial kernel, we also need to decide on sigma. The larger the sigma, the more non-linear is the boundary. And for the polynomial kernel, two other tuning parameters are scale, which helps us to normalize the pattern, and degree, which sets the degree for the kernel.\nPotential drawbacks\nSVM attempts to use a hyperplane to divide classes. However, when there are many noises in the observation, SVM probably cannot perform very well. Moreover, unlike tree and KNN, SVM does not allow us to classify observations to more than two levels. Also, we can’t get a probabilistic result like those generated by logistic models.\n\nShow code\n# Import raw data\ndata(penguins)\n\n# Check out the codebook by typing this in your console\n#?penguins\n\n# Create a new hybrid \n# And take a smaller sample (just for the purposes of this project)\nset.seed(253)\nnew_penguins <- penguins %>% \n  mutate(species = case_when(\n    species %in% c(\"Adelie\",\"Gentoo\") ~ \"Adelgen\",\n    species == \"Chinstrap\" ~ \"Chinstrap\"\n  )) %>% \n  group_by(species) %>% \n  sample_n(68)\n\n# preprocess the data\nnew_penguins <- new_penguins %>%\n  select(-year)%>%\n  mutate(island=as.factor(island))\n\n\n\nPart 2: Using SVM on Pengiuns Classification (Selected Variables)\n\nThis section, I will use the SVM model introduced before to predict the species of pengiuns. Here, I only use the bill_length and bill_width as predictors.\n\nKernel selection: linear, radial, or polynomial?\n\nThere are three methods for the SVM model: linear, radial, and poly. Using different methods will give us different types of separating hyperplanes. To choose the method that can best capture the differences between groups, I first studied the bill length and bill depth of different species by looking at the scatterplot. According to the graph, the boundary between Adelgen and Chinstrap seems to be an ellipse so my initial guess is that svmRadical would be a more proper method.\n\n\nShow code\nnew_penguins %>% \n  ggplot(aes(x=bill_length_mm,y=bill_depth_mm,color=species))+\n  geom_point()\n\n\n\n\n\n\nThen I built three SVM models using “svmRadial”, “svmLinear”, and “simply” separately to compare their cv accuracy and to decide which model generated the most accurate predictions. Below shows the cv accuracy of using “svmRadial”, “svmLinear”, and “svmPoly” respectively. Note that the caret package will automatically choose the optimal model tuning parameters, where optimal is defined as values that maximize the cv accuracy (Campbell,2020). As we can see, the cv accuracies of the models built using “svmRadial” and “svmPoly” are the same and both are higher than the cv accuracy generated by using “svmLinear”: on average, there is 0.9619 chance to guess the species of a penguin right using these two model. Integrating my previous guess based on the scatterplot, I would choose to use “svmRadial”. Now the problem is what tuning parameter to choose.\n\n\nShow code\nset.seed(253)\nsvmR <- train(\n     species~ bill_length_mm + bill_depth_mm,\n    data = new_penguins,\n    method = \"svmRadial\",\n    trControl = trainControl(method = \"cv\", number = 10, classProbs = TRUE),\n    tuneLength = 10,\n    metric = \"Accuracy\",\n    na.action = na.omit\n)\nsvmR$results %>% filter((sigma==svmR$bestTune$sigma) & (C==svmR$bestTune$C) )\n\n\n     sigma   C  Accuracy     Kappa AccuracySD   KappaSD\n1 3.663707 0.5 0.9547619 0.9095238 0.05549884 0.1109977\n\n\nShow code\nset.seed(253)\nsvmL <- train(\n     species~ bill_length_mm + bill_depth_mm,\n    data = new_penguins,\n    method = \"svmLinear\",\n    trControl = trainControl(method = \"cv\", number = 10),\n    tuneLength = 10,\n    metric = \"Accuracy\",\n    na.action = na.omit\n)\nsvmL$results %>% filter(C==svmL$bestTune$C)\n\n\n  C  Accuracy     Kappa AccuracySD   KappaSD\n1 1 0.8357143 0.6714286 0.08357863 0.1671573\n\n\nShow code\nset.seed(253)\nsvmP <- train(\n    species~ bill_length_mm + bill_depth_mm,\n    data = new_penguins,\n    method = \"svmPoly\",\n    trControl = trainControl(method = \"cv\", number = 10),\n    tuneLength = 4,\n    metric = \"Accuracy\",\n    na.action = na.omit\n)\n\n\n\n\nShow code\nsvmP$results %>% filter((degree==svmP$bestTune$degree)&(C==svmP$bestTune$C)&(scale==svmP$bestTune$scale))\n\n\n  degree scale   C  Accuracy     Kappa AccuracySD   KappaSD\n1      3     1 0.5 0.9619048 0.9238095 0.05634362 0.1126872\n\n\nThe model suggests the best tuning parameter is when sigma = 3.663707 and cost = 0.5. As we can see from the graph, when we choose tuning parameter cost = 0.5, the cv accuracy is indeed the highest.\n\n\nShow code\nsvmR$bestTune\n\n\n     sigma   C\n2 3.663707 0.5\nShow code\nplot(svmR)\n\n\n\n\nComparision with the logistic model\n\nI first looked at the confusion matrix of both models. First of all, for both models, P-Value [Acc > NIR] are small, suggesting that both model perform better than we doing a random guess. However, both sensitivity and specificity using “svmRadial” model are higher and the in-sample accuracy is also higher for “svmRadial” model (0.99>0.85). Therefore, it seems that “svmRadial” model does a better job for in-sample prediction, which is also supported by the ROC graph. The black line is the ROC for my SVM model and the red curve is the ROC for logistic model. The area under the curve for SVM model is bigger, showing that using SVM model, we has a higher probability to correctly identify Adelgen penguins as Adelgen, Chinstrap penguins as Chinstrap.\n\n\nShow code\nset.seed(253)\nlogistic_model <- train(\n    species ~ bill_length_mm+bill_depth_mm,\n    data = new_penguins,\n    method = \"glm\",\n    family = \"binomial\",\n    trControl = trainControl(method = \"cv\", number = 10),\n    metric = \"Accuracy\",\n    na.action = na.omit\n)\n\n\n\n\nShow code\npredict_data <- na.omit(svmR$trainingData)\n#p1<-predict(svmR,predict_data)\n#roc(response=predict_data$.outcome,pred=as.numeric(p1),plot=TRUE, legacy.axes=TRUE)\nclassifications_svmR <- predict(svmR, newdata = predict_data, type = \"raw\")\nconfusionMatrix(data = classifications_svmR, \n  reference = as.factor(predict_data$.outcome), \n  positive = \"Adelgen\")\n\n\nConfusion Matrix and Statistics\n\n           Reference\nPrediction  Adelgen Chinstrap\n  Adelgen        67         1\n  Chinstrap       1        67\n                                          \n               Accuracy : 0.9853          \n                 95% CI : (0.9479, 0.9982)\n    No Information Rate : 0.5             \n    P-Value [Acc > NIR] : <2e-16          \n                                          \n                  Kappa : 0.9706          \n                                          \n Mcnemar's Test P-Value : 1               \n                                          \n            Sensitivity : 0.9853          \n            Specificity : 0.9853          \n         Pos Pred Value : 0.9853          \n         Neg Pred Value : 0.9853          \n             Prevalence : 0.5000          \n         Detection Rate : 0.4926          \n   Detection Prevalence : 0.5000          \n      Balanced Accuracy : 0.9853          \n                                          \n       'Positive' Class : Adelgen         \n                                          \n\n\nShow code\nclassifications_log <- predict(logistic_model, newdata = predict_data, type = \"raw\")\nconfusionMatrix(data = classifications_log, \n  reference = as.factor(predict_data$.outcome), \n  positive = \"Adelgen\")\n\n\nConfusion Matrix and Statistics\n\n           Reference\nPrediction  Adelgen Chinstrap\n  Adelgen        60        12\n  Chinstrap       8        56\n                                          \n               Accuracy : 0.8529          \n                 95% CI : (0.7821, 0.9078)\n    No Information Rate : 0.5             \n    P-Value [Acc > NIR] : <2e-16          \n                                          \n                  Kappa : 0.7059          \n                                          \n Mcnemar's Test P-Value : 0.5023          \n                                          \n            Sensitivity : 0.8824          \n            Specificity : 0.8235          \n         Pos Pred Value : 0.8333          \n         Neg Pred Value : 0.8750          \n             Prevalence : 0.5000          \n         Detection Rate : 0.4412          \n   Detection Prevalence : 0.5294          \n      Balanced Accuracy : 0.8529          \n                                          \n       'Positive' Class : Adelgen         \n                                          \n\n\nShow code\np1<-predict(svmR,predict_data)\nroc(response=predict_data$.outcome,pred=as.numeric(p1),plot=TRUE, legacy.axes=TRUE)\n\n\n\nCall:\nroc.default(response = predict_data$.outcome, predictor = as.numeric(p1),     plot = TRUE, legacy.axes = TRUE)\n\nData: as.numeric(p1) in 68 controls (predict_data$.outcome Adelgen) < 68 cases (predict_data$.outcome Chinstrap).\nArea under the curve: 0.9853\nShow code\npar(new=TRUE)\nroc(response=predict_data$.outcome,pre=fitted(logistic_model),plot=TRUE,legacy.axes=TRUE,col=2)\n\n\n\n\nCall:\nroc.default(response = predict_data$.outcome, predictor = fitted(logistic_model),     plot = TRUE, legacy.axes = TRUE, col = 2)\n\nData: fitted(logistic_model) in 68 controls (predict_data$.outcome Adelgen) < 68 cases (predict_data$.outcome Chinstrap).\nArea under the curve: 0.9369\n\n\nThen I compared the cross-validation accuracy of these two models. First of all, the cv accuracies for both models are similar to the in-sample accuracy, suggesting that non of them overfits the data. The cv accuracy for the “svmRadial” model is 0.9619048 and the cv accuracy for the logistic model is 0.8440476, meaning that on average, we would have a higher chance to correctly classify a penguin using the “svmRadial” model.\n\n\nShow code\nsvmR$results %>% filter((sigma==svmR$bestTune$sigma) & (C==svmR$bestTune$C) )\n\n\n     sigma   C  Accuracy     Kappa AccuracySD   KappaSD\n1 3.663707 0.5 0.9547619 0.9095238 0.05549884 0.1109977\nShow code\nlogistic_model$results\n\n\n  parameter  Accuracy     Kappa AccuracySD   KappaSD\n1      none 0.8440476 0.6880952  0.1010854 0.2021708\n\n\nUsing both in-sample and cross-validation approaches, “svmRadial” seems to be a better model. Graphs below show the classification regions defined by my SVM model and the logistic model. As we can see, when we use the “svmRadial”, there are only about 3 cases in the sample that has been misclassified. However, when we used the logistic model, many cases are misclassified. These illustrations also help to prove that the “svmRadial” model might be a better classification model for this dataset.\n\n\nShow code\npenguins_data<-new_penguins %>% mutate(species=as.factor(species)) %>% select(species,bill_length_mm,bill_depth_mm)\n\n\n\n\nShow code\nsvmRfit=svm(as.factor(species)~bill_depth_mm+bill_length_mm, data=new_penguins, kernel=\"radial\", cost=0.5,gamma=3.663707)\nplot(svmRfit,penguins_data)\n\n\n\n\n\nShow code\nlogistic_model$finalModel\nclog <- function(x){\n  y=(1/1.2608)*(39.8641-0.3964*x)\n  return (y)\n}\n\n\n\n\nShow code\nnew_penguins %>% \n  ggplot(aes(x=bill_length_mm,y=bill_depth_mm,color=species))+\n  geom_point()+\n  geom_abline(intercept=39.8641/1.2608, slope = -0.3964/1.2608, linetype='solid', color='black')\n\n\n\n\n\nPart 3: Using SVM on Pengiuns Classification (All Variables)\n\nThis section, I will use the SVM model introduced before to predict the species of pengiuns. Here, I only use the bill_length and bill_width as predictors.\n\nKernel & tuning parameter selection\n\nSimilar to what I did in the previous part, to classify penguins using all the predictors in the dataset, I first compared the results using “svmLinear”, “svmRadial”, and “svmPloy” to decide which method generates better results. It turns out that “svmPoly” and “svmLinear” might be better methods than “svmRadial” because “svmRadial” has a lower cross-validation accuracy compare to the other two methods. The three graphs below show the relationship between the cost and the cross-validation accuracy using “svmRadial”, “svmLinear”, and “svmPloy” respectively. they visually demonstrate that on average, using “svmLinear” and “svmPloy” would have a higher chance to correctly classify the penguins. According to “svmPoly”, the best degree that optimizes the cv accuracy is 1, suggesting that a linear kernel might be a reasonable choice. Therefore, I would choose using “svmLinear” with a cost of 6.210526, which also makes sense by looking at the graph: when the cost is 6.210526, the cv accuracy is higher than using other tuning parameters.\n\n\nShow code\nset.seed(253)\nsvmR_f <- train(\n     species~ .,\n    data = new_penguins,\n    method = \"svmRadial\",\n    trControl = trainControl(method = \"cv\", number = 10),\n    tuneLength = 10,\n    metric = \"Accuracy\",\n    na.action = na.omit\n)\nsvmR_f$results %>% filter((sigma==svmR_f$bestTune$sigma) & (C==svmL$bestTune$C))\n\n\n      sigma C Accuracy     Kappa AccuracySD    KappaSD\n1 0.5265182 1 0.977381 0.9547619 0.03656397 0.07312793\n\n\nShow code\nset.seed(253)\nsvmL_f <- train(\n    species~ .,\n    data = new_penguins,\n    method = \"svmLinear\",\n    trControl = trainControl(method = \"cv\", number = 10, classProbs = TRUE),\n    tuneGrid = data.frame(C = seq(1, 100, length = 20)),\n    metric = \"Accuracy\",\n    na.action = na.omit\n)\nsvmL_f$results %>% filter(C==svmL_f$bestTune$C)\n\n\n         C  Accuracy     Kappa AccuracySD    KappaSD\n1 6.210526 0.9916667 0.9833333 0.02635231 0.05270463\n\n\nShow code\nset.seed(253)\nsvmP_f <- train(\n    species~ .,\n    data = new_penguins,\n    method = \"svmPoly\",\n    trControl = trainControl(method = \"cv\", number = 10),\n    tuneLength = 4,\n    metric = \"Accuracy\",\n    na.action = na.omit\n)\nsvmP_f$results %>% filter((degree==svmP_f$bestTune$degree) & (C==svmP_f$bestTune$C) & (scale==svmP_f$bestTune$scale))\n\n\n  degree scale    C  Accuracy     Kappa AccuracySD   KappaSD\n1      1     1 0.25 0.9928571 0.9857143  0.0225877 0.0451754\n\n\nShow code\nset.seed(253)\nsvmP_f2 <- train(\n    as.factor(species)~ .,\n    data = new_penguins,\n    method = \"svmPoly\",\n    trControl = trainControl(method = \"cv\", number = 10),\n    tuneGrid = data.frame(C=c(0.25,0.5,1,2),degree=1,scale=1),\n    metric = \"Accuracy\",\n    na.action = na.omit\n)\n\n\n\n\nShow code\nplot(svmR_f)\n\n\nShow code\nplot(svmL_f)\n\n\nShow code\nplot(svmP_f2)\n\n\n\n\nComparison with logistic model\n\nSimilarly, to compare my SVM model with the logistic model, I took in-sample and cross-validation approaches. Both models perform well for in-sample data. Logistic model is slightly better with in-sample classification, with an accuracy of 1 and both sensitivity and specificity being 1. Their ROC curves (showing below) are also very close together and the areas under the curve are both about 1. (The red one is the ROC curve by logistic model and the black line is the ROC curve for my SVM model.) However, my SVM model does have a higher cv accuracy (0.99>0.97). Therefore, my SVM model is slightly better in terms of classifying penguins outside of our sample.\n\n\nShow code\nset.seed(253)\nlogistic_f_model <- train(\n    as.factor(species) ~ .,\n    data = new_penguins,\n    method = \"glm\",\n    family = \"binomial\",\n    trControl = trainControl(method = \"cv\", number = 10),\n    metric = \"Accuracy\",\n    na.action = na.omit\n)\n\n\n\n\nShow code\npredict_data_f <- na.omit(svmL_f$trainingData)\nclassifications_svmL_f <- predict(svmL_f, newdata = predict_data_f, type = \"raw\")\nconfusionMatrix(data = classifications_svmL_f, \n  reference = as.factor(predict_data_f$.outcome), \n  positive = \"Adelgen\")\n\n\nConfusion Matrix and Statistics\n\n           Reference\nPrediction  Adelgen Chinstrap\n  Adelgen        67         1\n  Chinstrap       0        67\n                                          \n               Accuracy : 0.9926          \n                 95% CI : (0.9594, 0.9998)\n    No Information Rate : 0.5037          \n    P-Value [Acc > NIR] : <2e-16          \n                                          \n                  Kappa : 0.9852          \n                                          \n Mcnemar's Test P-Value : 1               \n                                          \n            Sensitivity : 1.0000          \n            Specificity : 0.9853          \n         Pos Pred Value : 0.9853          \n         Neg Pred Value : 1.0000          \n             Prevalence : 0.4963          \n         Detection Rate : 0.4963          \n   Detection Prevalence : 0.5037          \n      Balanced Accuracy : 0.9926          \n                                          \n       'Positive' Class : Adelgen         \n                                          \n\n\nShow code\nclassifications_log_f <- predict(logistic_f_model, newdata = predict_data_f, type = \"raw\")\nconfusionMatrix(data = classifications_log_f, \n  reference = as.factor(predict_data_f$.outcome), \n  positive = \"Adelgen\")\n\n\nConfusion Matrix and Statistics\n\n           Reference\nPrediction  Adelgen Chinstrap\n  Adelgen        67         0\n  Chinstrap       0        68\n                                    \n               Accuracy : 1         \n                 95% CI : (0.973, 1)\n    No Information Rate : 0.5037    \n    P-Value [Acc > NIR] : < 2.2e-16 \n                                    \n                  Kappa : 1         \n                                    \n Mcnemar's Test P-Value : NA        \n                                    \n            Sensitivity : 1.0000    \n            Specificity : 1.0000    \n         Pos Pred Value : 1.0000    \n         Neg Pred Value : 1.0000    \n             Prevalence : 0.4963    \n         Detection Rate : 0.4963    \n   Detection Prevalence : 0.4963    \n      Balanced Accuracy : 1.0000    \n                                    \n       'Positive' Class : Adelgen   \n                                    \n\n\nShow code\npredict_data2<-na.omit(svmL_f$trainingData)\np2<-predict(svmL_f,predict_data2)\nroc(response=predict_data2$.outcome,pred=as.numeric(p2),plot=TRUE, legacy.axes=TRUE)\n\n\n\nCall:\nroc.default(response = predict_data2$.outcome, predictor = as.numeric(p2),     plot = TRUE, legacy.axes = TRUE)\n\nData: as.numeric(p2) in 67 controls (predict_data2$.outcome Adelgen) < 68 cases (predict_data2$.outcome Chinstrap).\nArea under the curve: 0.9926\nShow code\npar(new=TRUE)\nroc(response=predict_data2$.outcome,pre=fitted(logistic_f_model),plot=TRUE,legacy.axes=TRUE,col=2)\n\n\n\n\nCall:\nroc.default(response = predict_data2$.outcome, predictor = fitted(logistic_f_model),     plot = TRUE, legacy.axes = TRUE, col = 2)\n\nData: fitted(logistic_f_model) in 67 controls (predict_data2$.outcome Adelgen) < 68 cases (predict_data2$.outcome Chinstrap).\nArea under the curve: 1\n\n\nShow code\nsvmL_f$results %>%filter(C==svmL_f$bestTune$C)\n\n\n         C  Accuracy     Kappa AccuracySD    KappaSD\n1 6.210526 0.9916667 0.9833333 0.02635231 0.05270463\nShow code\nlogistic_f_model$results\n\n\n  parameter Accuracy     Kappa AccuracySD    KappaSD\n1      none 0.977381 0.9547619 0.03656397 0.07312793\n\n\nPart 4: What the Model Shows?\n\nWith the SVM model obtained in the previous part, we can learn some themes about each classification. Below is a summary of average penguins’ features in each classification. As we can see, both from the summary, Chinstrap penguins tend to have longer and deeper bill compare to Adelgen penguins.\n\n\nAlso, Chinstrap seems to have shorter flippers in general, since the average flipper length for Chinstrap penguins is smaller. Moreover, by looking at the histograms of flipper length, we can also notice that within Adelgen penguins, the length of flippers has a wide range: some have very short flippers while some have very long flippers. Similarly, for body mass, Adelgen penguins tend to have a wider range of body mass while Chinstrap penguins tend to be less heavy in general.\n\n\nBy looking at the summary, all Chinstrap penguins live in Dream island while most Adelgen penguins live in Biscoe island with only a small group living in the other two islands.\n\n\nIn summary, based on the classification model in the previous part, we learn that Chinstrap penguins tend to have longer bill and deeper bill, shorter flipper, lower body mass, and live in Dream island; Adelgen penguins tend to have shorter and shallow bills, diverse length of the flipper and body mass, and mostly live in Biscoe island.\n\n\nShow code\nmy_model <- svm(as.factor(species)~.,new_penguins, kernel='linear',\n              cost=svmL_f$bestTune$C, decision.values=T)\nls=data.frame(predict(my_model,new_penguins))\n\npenguins_with_cluster <- new_penguins %>%\n  mutate(sex=as.numeric(as.factor(sex))-1)%>%\n  na.omit() \npenguins_with_cluster <- merge(penguins_with_cluster,ls, by.x = 0, by.y = 0) %>% rename(predicted=predict.my_model..new_penguins.\n) %>% \n  mutate(predicted=as.factor(predicted)) %>%\n  select(-Row.names)\n\npenguins_with_cluster<-penguins_with_cluster %>% mutate(islandDream=case_when(\n  island=='Dream'~ 1, island!='Dream'~0))%>%\n  mutate(islandBiscoe=case_when(\n  island=='Biscoe'~ 1, island!='Biscoe'~0)) %>%\n  mutate(islandTorgersen = case_when(\n  island=='Torgersen'~ 1,island!='Torgersen'~0))\n\n\n\n\nShow code\npenguins_with_cluster %>% group_by(as.factor(predicted)) %>% summarise_all(mean)\n\n\n# A tibble: 2 x 12\n  `as.factor(predicted)` species island bill_length_mm bill_depth_mm\n  <fct>                    <dbl>  <dbl>          <dbl>         <dbl>\n1 Adelgen                     NA     NA           43.1          16.5\n2 Chinstrap                   NA     NA           48.8          18.4\n# … with 7 more variables: flipper_length_mm <dbl>,\n#   body_mass_g <dbl>, sex <dbl>, predicted <dbl>, islandDream <dbl>,\n#   islandBiscoe <dbl>, islandTorgersen <dbl>\n\n\nShow code\npar(mfrow=c(2,2))\npenguins_with_cluster%>%\n  ggplot(aes(x=bill_depth_mm))+\n  geom_histogram()+\n  labs(title = \"Bill depth by classifications\")+\n  facet_wrap(~as.factor(predicted))\n\n\nShow code\npenguins_with_cluster%>%\n  ggplot(aes(x=bill_length_mm))+\n  geom_histogram()+\n  labs(title = \"Bill length by classifications\")+\n  facet_wrap(~predicted)\n\n\nShow code\npenguins_with_cluster%>%\n  ggplot(aes(x=flipper_length_mm))+\n  geom_histogram()+\n  labs(title = \"Flipper length by classifications\")+\n  facet_wrap(~predicted)\n\n\nShow code\npenguins_with_cluster%>%\n  ggplot(aes(x=body_mass_g))+\n  geom_histogram()+\n  labs(title = \"Body mass by classifications\")+\n  facet_wrap(~predicted)\n\n\n\n\n\nShow code\npenguins_with_cluster%>%\n  ggplot(aes(x=as.factor(sex)))+\n  geom_bar()+\n  labs(title = \"Sex by classifications\")+\n  facet_wrap(~predicted)\n\n\nShow code\npenguins_with_cluster%>%\n  ggplot(aes(x=as.factor(island)))+\n  geom_bar()+\n  labs(title = \"Island by classifications\")+\n  facet_wrap(~predicted)\n\n\n\n\n\nReference:\nJames, G., Witten, D., Hastie, T., & Tibshirani, R. (2017). An introduction to statistical learning with applications in R. New York: Springer.\nCampbell, J. (2020). SVM with caret. Retrieved December 13, 2020, from https://rpubs.com/uky994/593668\n\n\n\n",
    "preview": "projects/ml_p2/pengiuns.png",
    "last_modified": "2021-11-12T15:19:56-06:00",
    "input_file": {}
  },
  {
    "path": "projects/ml_p1/",
    "title": "Machine Leanring Project 1: Explore Price of Airbnb Listings in NYC",
    "description": "In this project, I used a Lasso model to explore influencers of price of Airbnb listing in NYC, evaluated the model, and improved it accordingly.",
    "author": [
      {
        "name": "Rita Liu",
        "url": {}
      }
    ],
    "date": "2020-11-23",
    "categories": [],
    "contents": "\n\nContents\nSetting Up\nPart 1: Process the data\nPart 2: Analyzepreliminary insights of price\nSelecting Variables Using Simple Lasso Model\nIs the model wrong?\nLinear regression using log(price)\n\nPart 3: Summarize\nApendix: GAM Model\n\nSetting Up\n\nShow code\nlibrary(ggplot2)  # for plots\nlibrary(GGally)   # for pairs plots\nlibrary(ggridges) # for joy plots\nlibrary(dplyr)    # for wrangling\nlibrary(caret)    # for machine learning algorithms\nlibrary(stringr)\nairbnb <- read.csv(\"https://www.macalester.edu/~ajohns24/data/NYC_airbnb_kaggle.csv\")\nnbhd <-read.csv(\"https://www.macalester.edu/~ajohns24/data/NYC_nbhd_kaggle.csv\")\n\n\n\n\nPart 1: Process the data\n\nI used NYC Airbnb data on Kaggle, joined two datasets, selected 5000 samples, and mutated varaibles for further use.\n\n\nShow code\nairbnb <- airbnb %>% rename(neighbourhood=neighbourhood_cleansed)\nairbnb <-airbnb %>% left_join(nbhd)\n#Process the data for model input\nset.seed(830)\nairbnb_sub<-airbnb%>%\n  filter(price<=1000) %>%\n  select(-id,-square_feet) %>%\n  sample_n(5000) %>%\n  mutate(\n    noamenities = str_count(amenities, ',') +1, # count the number of amenities in each house\n    host_response_rate = na.pass(as.numeric(str_remove(host_response_rate,\"%\"))), # convert host_response_rate from string to number\n    host_response_time=as.factor(host_response_time), # prepare all categorical variables for the model to use\n    host_is_superhost=as.factor(host_is_superhost),\n    neighbourhood=as.factor(neighbourhood),\n    property_type=as.factor(property_type),\n    room_type=as.factor(room_type),\n    bed_type=as.factor(bed_type),\n    instant_bookable = as.factor(instant_bookable),\n    cancellation_policy =as.factor(cancellation_policy),\n    calendar_updated=as.factor(calendar_updated),\n    room_type=as.factor(room_type),\n    is_location_exact=as.factor(is_location_exact),\n    host_has_profile_pic=as.factor(host_has_profile_pic),\n    is_business_travel_ready=as.factor(is_business_travel_ready),\n    require_guest_profile_picture=as.factor(require_guest_profile_picture),\n    neighbourhood_group=as.factor(neighbourhood_group)\n    ) %>%\n  select(-amenities)\n\n\n\nPart 2: Analyze\npreliminary insights of price\n\nShow code\nggplot(airbnb_sub,aes(x=price)) + \n  geom_histogram()\n\n\n\n\n\n\nAbove shows the distribution of listing price of Airbnb in our sample. The price of the Airbnb skews heavily to the right and a typical price for a Airbnb housing in the New York is around 125 dollars per night. The price for housing spreads out from 0 to 1000 dollars and is relatively disperse. There are probably a few outliers at the high end of the price.\n\n\nSelecting Variables Using Simple Lasso Model\n\nTo build and evaluate a predictive model of listing price, I first decide to use the LASSO model to choose the predictors I wish to include in the model since including all the variables available would generate a too complicated model for our purpose and the model tends to overfit. Lasso is chosen over the variable selection method not only because it is more computationally efficient, but also because it does not perform multiple testings when selecting the model so our results would not be overestimated.\n\n\n\nShow code\nset.seed(830)\n# Perform LASSO\nlasso_model <- train(\n    price ~ .,\n    data = airbnb_sub,\n    method = \"glmnet\",\n    tuneGrid = data.frame(alpha = 1, lambda = seq(0, 10, length = 100)),\n    trControl = trainControl(method = \"cv\", number = 10, selectionFunction = \"oneSE\"),\n    metric = \"MAE\",\n    na.action = na.omit\n)\n#coef(lasso_model$finalModel,lasso_model$bestTune$lambda)\n\n\n\n\nI have chosen the method “oneSE” over “best” because in this case, I want to have a relatively simple model. Here, lambda equals to 5.252525. The plot showing relationship between tuning parameter and MAE shows below. As we can see, the MAE for using a tuning parameter of 5.252525 is close to the lowest MAE. Therefore, this lambda value is reasonable. The variables remained after shrinkage are longitude, room_type, accommodates, bathrooms, bedrooms, availability_30, review_scores_rating, is_business_travel_ready, reviews_per_month, noamenities, and guests_included. Note that a few categorical variables like neighborhood and neighbourhood_group are removed because not all the levels remained after the shrinkage.\n\n\nShow code\nlasso_model$bestTune\n\n\n   alpha   lambda\n53     1 5.252525\nShow code\nplot(lasso_model)\n\n\n\n\n\nI then fit a linear regression using the variables selected by the LASSO model. On average, if we apply this model to a new set of data, a typical error would be 45.40 dollars. Here, R^2 is relatively low: only 53.97% of the listing price is explained by the model. I further study the residual plot to exam if the model is wrong.\n\n\nShow code\nmy_model <- train(\n    price ~ longitude+room_type + accommodates + bathrooms + bedrooms + availability_30+review_scores_rating+is_business_travel_ready+reviews_per_month+noamenities+guests_included,\n    data = airbnb_sub,\n    method = 'lm',\n    trControl = trainControl(method = 'cv', number = 10),\n    metric = \"MAE\",\n    na.action = na.omit\n)\n\n\n\n\nShow code\nsummary(my_model)\n\n\n\nCall:\nlm(formula = .outcome ~ ., data = dat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-307.27  -34.68   -5.84   23.35  774.77 \n\nCoefficients:\n                            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)               -4.340e+04  2.092e+03 -20.746  < 2e-16 ***\nlongitude                 -5.864e+02  2.828e+01 -20.738  < 2e-16 ***\n`room_typePrivate room`   -6.581e+01  2.984e+00 -22.054  < 2e-16 ***\n`room_typeShared room`    -9.311e+01  7.777e+00 -11.972  < 2e-16 ***\naccommodates               1.558e+01  1.062e+00  14.677  < 2e-16 ***\nbathrooms                  4.132e+01  3.348e+00  12.343  < 2e-16 ***\nbedrooms                   1.736e+01  2.321e+00   7.478 9.28e-14 ***\navailability_30            1.684e+00  1.432e-01  11.760  < 2e-16 ***\nreview_scores_rating       7.559e-01  1.486e-01   5.086 3.84e-07 ***\nis_business_travel_readyt  7.710e+00  4.522e+00   1.705   0.0883 .  \nreviews_per_month         -3.553e+00  7.441e-01  -4.775 1.86e-06 ***\nnoamenities                4.576e-01  1.800e-01   2.543   0.0110 *  \nguests_included            5.305e+00  1.255e+00   4.227 2.42e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 72.22 on 3835 degrees of freedom\nMultiple R-squared:  0.5397,    Adjusted R-squared:  0.5382 \nF-statistic: 374.7 on 12 and 3835 DF,  p-value: < 2.2e-16\n\n\nShow code\nmy_model$results\n\n\n  intercept     RMSE  Rsquared      MAE   RMSESD RsquaredSD    MAESD\n1      TRUE 72.36787 0.5396401 45.40293 8.248047 0.04084573 2.843008\n\nIs the model wrong?\n\nShow code\n# Combine residuals & predictions into data frame\nresult_df <- data.frame(resid = resid(my_model), fitted = fitted(my_model))\n\n# Residual plot\nggplot(result_df, aes(x = fitted, y = resid)) + \n  geom_point() + \n  geom_hline(yintercept = 0)\n\n\n\n\n\n\nAbove shows the residual plot of a simple regression model with selected predictors. The residual plot is not ideal because it is not balanced above and below zero. We can also observe a negative trend of the residuals. The plot is also heteroskedastic: the smaller the fitted value, the smaller the residuals are. One possible reason is that our outcome price skews heavily to the right. Therefore, I even the distribution of outcome by using log(price).\n\nLinear regression using log(price)\npreliminary insights of log(price)\n\nBelow is the graph showing distribution of log(price). As we can see, it is about symmetric and centered around 5. Using log(price) should help to generate a better model.\n\n\nShow code\nggplot(airbnb_sub,aes(x=log(price))) + \n  geom_histogram()\n\n\n\n\n\nActual Model: log(price) as the outcome\n\nShow code\nlog_model_data<-airbnb_sub %>%\n  select(price,longitude,room_type,accommodates,bathrooms,bedrooms,availability_30,review_scores_rating,is_business_travel_ready,reviews_per_month,noamenities,guests_included) %>%\n  mutate(logprice=log(price))%>%\n  select(-price) %>%\n  filter(logprice>-Inf)\n\n\n\n\nShow code\nlog_model <- train(\n    logprice ~ longitude+room_type + accommodates + bathrooms + bedrooms + availability_30+review_scores_rating+is_business_travel_ready+reviews_per_month+noamenities+guests_included,\n    data = log_model_data,\n    method = 'lm',\n    trControl = trainControl(method = 'cv', number = 10),\n    metric = \"MAE\",\n    na.action = na.omit\n)\n\n\n\n\nShow code\nlog_model$results\n\n\n  intercept      RMSE  Rsquared       MAE     RMSESD RsquaredSD\n1      TRUE 0.3860566 0.6310932 0.2914568 0.01727447 0.02462859\n       MAESD\n1 0.01570924\n\n\nShow code\n# Combine residuals & predictions into data frame\nresult_df3 <- data.frame(resid = resid(log_model), fitted = fitted(log_model))\n\n# Residual plot\nggplot(result_df3, aes(x = fitted, y = resid)) + \n  geom_point() + \n  geom_hline(yintercept = 0)\n\n\n\n\n\n\nWith the log model, R^2 improved: now 63.11% of y variable is explained by the model. The residual plot also improved: the residuals are now balanced around zero and seem to be randomly distributed. The residual plot is not heteroskedastic anymore. Therefore, log model probably is a better model in predicting the price.\n\n\n\nThrough the model building process, I choose the least square regression over the KNN and GAM model because the least square model is easier to interpret. However, the GAM model is indeed better if the relationship between the outcome and predictors are nonlinear. Therefore, I build a gam model using the same predictors selected by the LASSO model. The detailed codes and results can be found in the appendix. The gam model has an MAE of 43.71 dollars and a r^2 of 56.50%, not significantly outperform the least square regression. As a result, I choose the least square model using log(price) as the outcome.\n\n\nPart 3: Summarize\n\nShow code\nsummary(log_model)\n\n\n\nCall:\nlm(formula = .outcome ~ ., data = dat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.4487 -0.2395 -0.0009  0.2234  2.4892 \n\nCoefficients:\n                            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)               -3.024e+02  1.117e+01 -27.065  < 2e-16 ***\nlongitude                 -4.142e+00  1.510e-01 -27.427  < 2e-16 ***\n`room_typePrivate room`   -5.852e-01  1.593e-02 -36.739  < 2e-16 ***\n`room_typeShared room`    -1.016e+00  4.149e-02 -24.483  < 2e-16 ***\naccommodates               7.393e-02  5.667e-03  13.046  < 2e-16 ***\nbathrooms                  1.080e-01  1.786e-02   6.048 1.60e-09 ***\nbedrooms                   5.886e-02  1.240e-02   4.746 2.15e-06 ***\navailability_30            8.692e-03  7.639e-04  11.377  < 2e-16 ***\nreview_scores_rating       5.618e-03  7.930e-04   7.084 1.66e-12 ***\nis_business_travel_readyt  3.051e-02  2.413e-02   1.264    0.206    \nreviews_per_month         -1.623e-02  3.970e-03  -4.088 4.45e-05 ***\nnoamenities                3.965e-03  9.604e-04   4.129 3.73e-05 ***\nguests_included            3.560e-02  6.695e-03   5.317 1.12e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3853 on 3832 degrees of freedom\nMultiple R-squared:  0.6332,    Adjusted R-squared:  0.6321 \nF-statistic: 551.3 on 12 and 3832 DF,  p-value: < 2.2e-16\nShow code\nround(exp(log_model$finalModel$coefficients),3)\n\n\n              (Intercept)                 longitude \n                    0.000                     0.016 \n  `room_typePrivate room`    `room_typeShared room` \n                    0.557                     0.362 \n             accommodates                 bathrooms \n                    1.077                     1.114 \n                 bedrooms           availability_30 \n                    1.061                     1.009 \n     review_scores_rating is_business_travel_readyt \n                    1.006                     1.031 \n        reviews_per_month               noamenities \n                    0.984                     1.004 \n          guests_included \n                    1.036 \n\n\n\nAirbnb price is positively associated with the following factors: the number of accommodates, bathrooms, bedrooms, amenities, and guests included, as well as its availability in 30 days, whether it is ready for business travel, and the rating of review scores. Among them, the number of accommodates and bathrooms have the strongest and significant impact. With one more accommodate and bathroom, the price on average would be 1.077 and 1.114 times higher respectively, holding other variables constant.\n\n\n\nOn the other hand, longitude, private room, shared room, and reviews per month are negatively associated with price. Among them, the strongest and the most significant predictors to listing price seems to be longitude and room types. If the longitude increases by 1 degree, the price of Airbnb will, on average, be 0.016 times less, holding other variables constant. For room types, compared to entire home or apartment, if the room types are private rooms or shared rooms, the price, on average, would be 0.557 and 0.362 times lower respectively, holding other variables constant. Below is the graph picturing the relationship between room types and price. According to the box plot, it seems that private rooms and shared rooms typically are cheaper than the entire home or apartment.\n\n\nShow code\nggplot(airbnb_sub,aes(y=price,x=room_type)) +\n  geom_boxplot()\n\n\n\n\n\nApendix: GAM Model\n\nShow code\n    ggplot(airbnb_sub,aes(y=price,x=bathrooms))+\n      geom_point()\n\n\n\n\n\n\nI do notice that not all the variables have linear relationship with the outcome. For example, above is the plot showing the relationship between the number of bathrooms and the price. Clearly, this is not a linear relationship. In this case, GAM might be a better model when using to predict the outcome. Therefore, I fitted a GAM model using the same set of predictors.\n\n\nShow code\n#process data\ngam_model_data <- airbnb_sub %>% \n      select(price,longitude,room_type,accommodates,bathrooms,bedrooms,availability_30,number_of_reviews,review_scores_rating,is_business_travel_ready,reviews_per_month,noamenities,guests_included) \n     \ngam_model_data <- data.frame(model.matrix(~ . - 1, gam_model_data))\n    \ngam_model <- train(\n    price ~ longitude+room_typeEntire.home.apt+room_typePrivate.room+room_typeShared.room+accommodates+bathrooms+bedrooms+availability_30+number_of_reviews+review_scores_rating+is_business_travel_readyt+reviews_per_month+noamenities+guests_included,\n    data = gam_model_data,\n    method = 'gamLoess',\n    tuneGrid = data.frame(span = seq(0.1, 1, length = 10), degree = 1),\n    trControl = trainControl(method = \"cv\", number = 10, selectionFunction = \"best\"),\n    metric = \"MAE\",\n    na.action = na.omit\n)\n\ngam_model$results %>% filter(span==gam_model$bestTune$span)\n\n\n  span degree    RMSE  Rsquared      MAE   RMSESD RsquaredSD    MAESD\n1  0.7      1 70.1644 0.5650253 43.71761 8.889075 0.05344549 3.331914\n\n\nWith the GAM model, the R^2 is 0.565, meaning that 56.5% listing price are explained by the model; The MAE is 43.71 dollars: when we apply this model to a new set of data, we would expect the prediction to be off by 43.84 dollars. These results are not better than the least sqaure model.\n\n\n\nShow code\n# Combine residuals & predictions into data frame\nresult_df2 <- data.frame(resid = resid(gam_model), fitted = fitted(gam_model))\n\n# Residual plot\nggplot(result_df2, aes(x = fitted, y = resid)) + \n  geom_point() + \n  geom_hline(yintercept = 0)\n\n\n\n\n\n\nAbove shows the residual plot of GAM model. The residual plot shows that this model might be wrong too since it is not balanced aroudn zero and the residuals are not random.\n\n\n\n\n",
    "preview": "projects/ml_p1/airbnb.png",
    "last_modified": "2021-11-06T15:00:32-05:00",
    "input_file": {}
  }
]
