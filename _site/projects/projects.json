[
  {
    "path": "projects/ml_p2/",
    "title": "Machine Leanring Project 2: SVM Summary and Its Application on Penguin Species",
    "description": "In this project, I will give a brief overview of a new classification algorithm support vector machines (SVM) and use it to predict Penguin Species using palmerpenguins data in R.",
    "author": [
      {
        "name": "Rita Liu",
        "url": {}
      }
    ],
    "date": "2020-12-11",
    "categories": [],
    "contents": "\n\nContents\nSetting Up\nPart 1: SVM OverviewPlacement within the machine learning workflow\nDefinition\nTuning\nPotential drawbacks\n\nPart 2: Using SVM on Pengiuns Classification (Selected Variables)Kernel selection: linear, radial, or polynomial?\nComparision with the logistic model\n\nPart 3: Using SVM on Pengiuns Classification (All Variables)Kernel & tuning parameter selection\nComparison with logistic model\n\nPart 4: What the Model Shows?\nReference:\n\n\nSetting Up\n\nShow code\nlibrary(ggplot2)\nlibrary(caret)\nlibrary(e1071)\nlibrary(ROCR)\nlibrary(dplyr)\nlibrary(palmerpenguins)\nlibrary(pROC)\n\n\n\nPart 1: SVM Overview\nPlacement within the machine learning workflow\nSVM vs Logistic Regression: SMV is also a parametric model and it will be more advantageous when the boundaries between different catergories are not linear.\nSVM vs KNN: Both of the model allow non-linear boundaries for classification. However, when we have many predictors or features, SVM would perform better than KNN because KNN suffers from curse of dimensionality.\nSVM vs tree: I personally think that SVM is a less greedy algorithm comparing to tree model because tree makes best splits without considering the future split. However, for SVM, we won’t have such a concern.\nDefinition\nIn short, the SVM model uses a separating hyperplane to classify observations.\nA separating hyperplane \\(f(x_1,x_2,...,x_n)\\) is a plane in space which can separate different levels of the observations based on the features. The observations fall into two classes, we use -1 to represent one class and 1 to represents the other class,that is \\(y_{i} ∈ \\{-1,1\\}\\). As a result, a hyperplane has the following property: \\(y_{i}*f(x_1,x_2,...,x_n) > 0\\).\n\nHowever, there are many hyperplanes that can separate the observations, as we can see from the figure below.  Since we want to choose the plane that best separates two classes, we want the plane to have the farthest minimum distance or margin from the observations. In the language of math, that is, \\(y_{i}*f(x_1,x_2,...,x_n) > = M\\), where M is the farthest minimum distance from the hyperplane to the observations.\nAs we can see from the figure below, three training observations are on the dashed line, meaning that the distance between these three observations and the separating hyperplane is the margin. Those points are support vectors, because if they move, the hyperplane would change. \nNot all observations can be perfectly separated into two classes by a plane and we could allow misclassifying of some a few observations by adding slack variables \\(ε\\) and the hyperplane satisfies \\(y_{i}*f(x_1,x_2,...,x_n) > = M*(1-ε_i)\\). When \\(ε_i=0\\), the observation is on the correct side of the margin; when \\(ε_i>0\\), the observation is on the wrong side of the margin; and when \\(ε_i>1\\), it’s on the wrong side of the plane. For example, as the graph below demonstrates, observation 12 and 11 would have a slack variable bigger than 1 because they are on the wrong side of the hyperplane.\n\n\nHyperplane function \\(f(x_i)\\) can have different forms or take in different kernels which allow us to have non-linear boundaries. For example, in the graph below, the figure on the left uses a polynomial kernel and the figure on the right uses a radial kernel.\nTuning\nThe tuning parameter for SVM is cost, which is defined as the sum of \\(εi\\). When the cost is big, the width of the margin will be wilder and many points can violate the margin. While when the cost is small, the width of the margin will be small and fewer misclassifications will be tolerated.\nBesides cost C, there are other tuning parameters according to what kernels we use. If we use the radial kernel, we also need to decide on sigma. The larger the sigma, the more non-linear is the boundary. And for the polynomial kernel, two other tuning parameters are scale, which helps us to normalize the pattern, and degree, which sets the degree for the kernel.\nPotential drawbacks\nSVM attempts to use a hyperplane to divide classes. However, when there are many noises in the observation, SVM probably cannot perform very well. Moreover, unlike tree and KNN, SVM does not allow us to classify observations to more than two levels. Also, we can’t get a probabilistic result like those generated by logistic models.\n\nShow code\n# Import raw data\ndata(penguins)\n\n# Check out the codebook by typing this in your console\n#?penguins\n\n# Create a new hybrid \n# And take a smaller sample (just for the purposes of this project)\nset.seed(253)\nnew_penguins <- penguins %>% \n  mutate(species = case_when(\n    species %in% c(\"Adelie\",\"Gentoo\") ~ \"Adelgen\",\n    species == \"Chinstrap\" ~ \"Chinstrap\"\n  )) %>% \n  group_by(species) %>% \n  sample_n(68)\n\n# preprocess the data\nnew_penguins <- new_penguins %>%\n  select(-year)%>%\n  mutate(island=as.factor(island))\n\n\n\nPart 2: Using SVM on Pengiuns Classification (Selected Variables)\n\nThis section, I will use the SVM model introduced before to predict the species of pengiuns. Here, I only use the bill_length and bill_width as predictors.\n\nKernel selection: linear, radial, or polynomial?\n\nThere are three methods for the SVM model: linear, radial, and poly. Using different methods will give us different types of separating hyperplanes. To choose the method that can best capture the differences between groups, I first studied the bill length and bill depth of different species by looking at the scatterplot. According to the graph, the boundary between Adelgen and Chinstrap seems to be an ellipse so my initial guess is that svmRadical would be a more proper method.\n\n\nShow code\nnew_penguins %>% \n  ggplot(aes(x=bill_length_mm,y=bill_depth_mm,color=species))+\n  geom_point()\n\n\n\n\n\n\nThen I built three SVM models using “svmRadial”, “svmLinear”, and “simply” separately to compare their cv accuracy and to decide which model generated the most accurate predictions. Below shows the cv accuracy of using “svmRadial”, “svmLinear”, and “svmPoly” respectively. Note that the caret package will automatically choose the optimal model tuning parameters, where optimal is defined as values that maximize the cv accuracy (Campbell,2020). As we can see, the cv accuracies of the models built using “svmRadial” and “svmPoly” are the same and both are higher than the cv accuracy generated by using “svmLinear”: on average, there is 0.9619 chance to guess the species of a penguin right using these two model. Integrating my previous guess based on the scatterplot, I would choose to use “svmRadial”. Now the problem is what tuning parameter to choose.\n\n\nShow code\nset.seed(253)\nsvmR <- train(\n     species~ bill_length_mm + bill_depth_mm,\n    data = new_penguins,\n    method = \"svmRadial\",\n    trControl = trainControl(method = \"cv\", number = 10, classProbs = TRUE),\n    tuneLength = 10,\n    metric = \"Accuracy\",\n    na.action = na.omit\n)\nsvmR$results %>% filter((sigma==svmR$bestTune$sigma) & (C==svmR$bestTune$C) )\n\n\n     sigma   C  Accuracy     Kappa AccuracySD   KappaSD\n1 3.663707 0.5 0.9547619 0.9095238 0.05549884 0.1109977\n\n\nShow code\nset.seed(253)\nsvmL <- train(\n     species~ bill_length_mm + bill_depth_mm,\n    data = new_penguins,\n    method = \"svmLinear\",\n    trControl = trainControl(method = \"cv\", number = 10),\n    tuneLength = 10,\n    metric = \"Accuracy\",\n    na.action = na.omit\n)\nsvmL$results %>% filter(C==svmL$bestTune$C)\n\n\n  C  Accuracy     Kappa AccuracySD   KappaSD\n1 1 0.8357143 0.6714286 0.08357863 0.1671573\n\n\nShow code\nset.seed(253)\nsvmP <- train(\n    species~ bill_length_mm + bill_depth_mm,\n    data = new_penguins,\n    method = \"svmPoly\",\n    trControl = trainControl(method = \"cv\", number = 10),\n    tuneLength = 4,\n    metric = \"Accuracy\",\n    na.action = na.omit\n)\n\n\n\n\nShow code\nsvmP$results %>% filter((degree==svmP$bestTune$degree)&(C==svmP$bestTune$C)&(scale==svmP$bestTune$scale))\n\n\n  degree scale   C  Accuracy     Kappa AccuracySD   KappaSD\n1      3     1 0.5 0.9619048 0.9238095 0.05634362 0.1126872\n\n\nThe model suggests the best tuning parameter is when sigma = 3.663707 and cost = 0.5. As we can see from the graph, when we choose tuning parameter cost = 0.5, the cv accuracy is indeed the highest.\n\n\nShow code\nsvmR$bestTune\n\n\n     sigma   C\n2 3.663707 0.5\nShow code\nplot(svmR)\n\n\n\n\nComparision with the logistic model\n\nI first looked at the confusion matrix of both models. First of all, for both models, P-Value [Acc > NIR] are small, suggesting that both model perform better than we doing a random guess. However, both sensitivity and specificity using “svmRadial” model are higher and the in-sample accuracy is also higher for “svmRadial” model (0.99>0.85). Therefore, it seems that “svmRadial” model does a better job for in-sample prediction, which is also supported by the ROC graph. The black line is the ROC for my SVM model and the red curve is the ROC for logistic model. The area under the curve for SVM model is bigger, showing that using SVM model, we has a higher probability to correctly identify Adelgen penguins as Adelgen, Chinstrap penguins as Chinstrap.\n\n\nShow code\nset.seed(253)\nlogistic_model <- train(\n    species ~ bill_length_mm+bill_depth_mm,\n    data = new_penguins,\n    method = \"glm\",\n    family = \"binomial\",\n    trControl = trainControl(method = \"cv\", number = 10),\n    metric = \"Accuracy\",\n    na.action = na.omit\n)\n\n\n\n\nShow code\npredict_data <- na.omit(svmR$trainingData)\n#p1<-predict(svmR,predict_data)\n#roc(response=predict_data$.outcome,pred=as.numeric(p1),plot=TRUE, legacy.axes=TRUE)\nclassifications_svmR <- predict(svmR, newdata = predict_data, type = \"raw\")\nconfusionMatrix(data = classifications_svmR, \n  reference = as.factor(predict_data$.outcome), \n  positive = \"Adelgen\")\n\n\nConfusion Matrix and Statistics\n\n           Reference\nPrediction  Adelgen Chinstrap\n  Adelgen        67         1\n  Chinstrap       1        67\n                                          \n               Accuracy : 0.9853          \n                 95% CI : (0.9479, 0.9982)\n    No Information Rate : 0.5             \n    P-Value [Acc > NIR] : <2e-16          \n                                          \n                  Kappa : 0.9706          \n                                          \n Mcnemar's Test P-Value : 1               \n                                          \n            Sensitivity : 0.9853          \n            Specificity : 0.9853          \n         Pos Pred Value : 0.9853          \n         Neg Pred Value : 0.9853          \n             Prevalence : 0.5000          \n         Detection Rate : 0.4926          \n   Detection Prevalence : 0.5000          \n      Balanced Accuracy : 0.9853          \n                                          \n       'Positive' Class : Adelgen         \n                                          \n\n\nShow code\nclassifications_log <- predict(logistic_model, newdata = predict_data, type = \"raw\")\nconfusionMatrix(data = classifications_log, \n  reference = as.factor(predict_data$.outcome), \n  positive = \"Adelgen\")\n\n\nConfusion Matrix and Statistics\n\n           Reference\nPrediction  Adelgen Chinstrap\n  Adelgen        60        12\n  Chinstrap       8        56\n                                          \n               Accuracy : 0.8529          \n                 95% CI : (0.7821, 0.9078)\n    No Information Rate : 0.5             \n    P-Value [Acc > NIR] : <2e-16          \n                                          \n                  Kappa : 0.7059          \n                                          \n Mcnemar's Test P-Value : 0.5023          \n                                          \n            Sensitivity : 0.8824          \n            Specificity : 0.8235          \n         Pos Pred Value : 0.8333          \n         Neg Pred Value : 0.8750          \n             Prevalence : 0.5000          \n         Detection Rate : 0.4412          \n   Detection Prevalence : 0.5294          \n      Balanced Accuracy : 0.8529          \n                                          \n       'Positive' Class : Adelgen         \n                                          \n\n\nShow code\np1<-predict(svmR,predict_data)\nroc(response=predict_data$.outcome,pred=as.numeric(p1),plot=TRUE, legacy.axes=TRUE)\n\n\n\nCall:\nroc.default(response = predict_data$.outcome, predictor = as.numeric(p1),     plot = TRUE, legacy.axes = TRUE)\n\nData: as.numeric(p1) in 68 controls (predict_data$.outcome Adelgen) < 68 cases (predict_data$.outcome Chinstrap).\nArea under the curve: 0.9853\nShow code\npar(new=TRUE)\nroc(response=predict_data$.outcome,pre=fitted(logistic_model),plot=TRUE,legacy.axes=TRUE,col=2)\n\n\n\n\nCall:\nroc.default(response = predict_data$.outcome, predictor = fitted(logistic_model),     plot = TRUE, legacy.axes = TRUE, col = 2)\n\nData: fitted(logistic_model) in 68 controls (predict_data$.outcome Adelgen) < 68 cases (predict_data$.outcome Chinstrap).\nArea under the curve: 0.9369\n\n\nThen I compared the cross-validation accuracy of these two models. First of all, the cv accuracies for both models are similar to the in-sample accuracy, suggesting that non of them overfits the data. The cv accuracy for the “svmRadial” model is 0.9619048 and the cv accuracy for the logistic model is 0.8440476, meaning that on average, we would have a higher chance to correctly classify a penguin using the “svmRadial” model.\n\n\nShow code\nsvmR$results %>% filter((sigma==svmR$bestTune$sigma) & (C==svmR$bestTune$C) )\n\n\n     sigma   C  Accuracy     Kappa AccuracySD   KappaSD\n1 3.663707 0.5 0.9547619 0.9095238 0.05549884 0.1109977\nShow code\nlogistic_model$results\n\n\n  parameter  Accuracy     Kappa AccuracySD   KappaSD\n1      none 0.8440476 0.6880952  0.1010854 0.2021708\n\n\nUsing both in-sample and cross-validation approaches, “svmRadial” seems to be a better model. Graphs below show the classification regions defined by my SVM model and the logistic model. As we can see, when we use the “svmRadial”, there are only about 3 cases in the sample that has been misclassified. However, when we used the logistic model, many cases are misclassified. These illustrations also help to prove that the “svmRadial” model might be a better classification model for this dataset.\n\n\nShow code\npenguins_data<-new_penguins %>% mutate(species=as.factor(species)) %>% select(species,bill_length_mm,bill_depth_mm)\n\n\n\n\nShow code\nsvmRfit=svm(as.factor(species)~bill_depth_mm+bill_length_mm, data=new_penguins, kernel=\"radial\", cost=0.5,gamma=3.663707)\nplot(svmRfit,penguins_data)\n\n\n\n\n\nShow code\nlogistic_model$finalModel\nclog <- function(x){\n  y=(1/1.2608)*(39.8641-0.3964*x)\n  return (y)\n}\n\n\n\n\nShow code\nnew_penguins %>% \n  ggplot(aes(x=bill_length_mm,y=bill_depth_mm,color=species))+\n  geom_point()+\n  geom_abline(intercept=39.8641/1.2608, slope = -0.3964/1.2608, linetype='solid', color='black')\n\n\n\n\n\nPart 3: Using SVM on Pengiuns Classification (All Variables)\n\nThis section, I will use the SVM model introduced before to predict the species of pengiuns. Here, I only use the bill_length and bill_width as predictors.\n\nKernel & tuning parameter selection\n\nSimilar to what I did in the previous part, to classify penguins using all the predictors in the dataset, I first compared the results using “svmLinear”, “svmRadial”, and “svmPloy” to decide which method generates better results. It turns out that “svmPoly” and “svmLinear” might be better methods than “svmRadial” because “svmRadial” has a lower cross-validation accuracy compare to the other two methods. The three graphs below show the relationship between the cost and the cross-validation accuracy using “svmRadial”, “svmLinear”, and “svmPloy” respectively. they visually demonstrate that on average, using “svmLinear” and “svmPloy” would have a higher chance to correctly classify the penguins. According to “svmPoly”, the best degree that optimizes the cv accuracy is 1, suggesting that a linear kernel might be a reasonable choice. Therefore, I would choose using “svmLinear” with a cost of 6.210526, which also makes sense by looking at the graph: when the cost is 6.210526, the cv accuracy is higher than using other tuning parameters.\n\n\nShow code\nset.seed(253)\nsvmR_f <- train(\n     species~ .,\n    data = new_penguins,\n    method = \"svmRadial\",\n    trControl = trainControl(method = \"cv\", number = 10),\n    tuneLength = 10,\n    metric = \"Accuracy\",\n    na.action = na.omit\n)\nsvmR_f$results %>% filter((sigma==svmR_f$bestTune$sigma) & (C==svmL$bestTune$C))\n\n\n      sigma C Accuracy     Kappa AccuracySD    KappaSD\n1 0.5265182 1 0.977381 0.9547619 0.03656397 0.07312793\n\n\nShow code\nset.seed(253)\nsvmL_f <- train(\n    species~ .,\n    data = new_penguins,\n    method = \"svmLinear\",\n    trControl = trainControl(method = \"cv\", number = 10, classProbs = TRUE),\n    tuneGrid = data.frame(C = seq(1, 100, length = 20)),\n    metric = \"Accuracy\",\n    na.action = na.omit\n)\nsvmL_f$results %>% filter(C==svmL_f$bestTune$C)\n\n\n         C  Accuracy     Kappa AccuracySD    KappaSD\n1 6.210526 0.9916667 0.9833333 0.02635231 0.05270463\n\n\nShow code\nset.seed(253)\nsvmP_f <- train(\n    species~ .,\n    data = new_penguins,\n    method = \"svmPoly\",\n    trControl = trainControl(method = \"cv\", number = 10),\n    tuneLength = 4,\n    metric = \"Accuracy\",\n    na.action = na.omit\n)\nsvmP_f$results %>% filter((degree==svmP_f$bestTune$degree) & (C==svmP_f$bestTune$C) & (scale==svmP_f$bestTune$scale))\n\n\n  degree scale    C  Accuracy     Kappa AccuracySD   KappaSD\n1      1     1 0.25 0.9928571 0.9857143  0.0225877 0.0451754\n\n\nShow code\nset.seed(253)\nsvmP_f2 <- train(\n    as.factor(species)~ .,\n    data = new_penguins,\n    method = \"svmPoly\",\n    trControl = trainControl(method = \"cv\", number = 10),\n    tuneGrid = data.frame(C=c(0.25,0.5,1,2),degree=1,scale=1),\n    metric = \"Accuracy\",\n    na.action = na.omit\n)\n\n\n\n\nShow code\nplot(svmR_f)\n\n\nShow code\nplot(svmL_f)\n\n\nShow code\nplot(svmP_f2)\n\n\n\n\nComparison with logistic model\n\nSimilarly, to compare my SVM model with the logistic model, I took in-sample and cross-validation approaches. Both models perform well for in-sample data. Logistic model is slightly better with in-sample classification, with an accuracy of 1 and both sensitivity and specificity being 1. Their ROC curves (showing below) are also very close together and the areas under the curve are both about 1. (The red one is the ROC curve by logistic model and the black line is the ROC curve for my SVM model.) However, my SVM model does have a higher cv accuracy (0.99>0.97). Therefore, my SVM model is slightly better in terms of classifying penguins outside of our sample.\n\n\nShow code\nset.seed(253)\nlogistic_f_model <- train(\n    as.factor(species) ~ .,\n    data = new_penguins,\n    method = \"glm\",\n    family = \"binomial\",\n    trControl = trainControl(method = \"cv\", number = 10),\n    metric = \"Accuracy\",\n    na.action = na.omit\n)\n\n\n\n\nShow code\npredict_data_f <- na.omit(svmL_f$trainingData)\nclassifications_svmL_f <- predict(svmL_f, newdata = predict_data_f, type = \"raw\")\nconfusionMatrix(data = classifications_svmL_f, \n  reference = as.factor(predict_data_f$.outcome), \n  positive = \"Adelgen\")\n\n\nConfusion Matrix and Statistics\n\n           Reference\nPrediction  Adelgen Chinstrap\n  Adelgen        67         1\n  Chinstrap       0        67\n                                          \n               Accuracy : 0.9926          \n                 95% CI : (0.9594, 0.9998)\n    No Information Rate : 0.5037          \n    P-Value [Acc > NIR] : <2e-16          \n                                          \n                  Kappa : 0.9852          \n                                          \n Mcnemar's Test P-Value : 1               \n                                          \n            Sensitivity : 1.0000          \n            Specificity : 0.9853          \n         Pos Pred Value : 0.9853          \n         Neg Pred Value : 1.0000          \n             Prevalence : 0.4963          \n         Detection Rate : 0.4963          \n   Detection Prevalence : 0.5037          \n      Balanced Accuracy : 0.9926          \n                                          \n       'Positive' Class : Adelgen         \n                                          \n\n\nShow code\nclassifications_log_f <- predict(logistic_f_model, newdata = predict_data_f, type = \"raw\")\nconfusionMatrix(data = classifications_log_f, \n  reference = as.factor(predict_data_f$.outcome), \n  positive = \"Adelgen\")\n\n\nConfusion Matrix and Statistics\n\n           Reference\nPrediction  Adelgen Chinstrap\n  Adelgen        67         0\n  Chinstrap       0        68\n                                    \n               Accuracy : 1         \n                 95% CI : (0.973, 1)\n    No Information Rate : 0.5037    \n    P-Value [Acc > NIR] : < 2.2e-16 \n                                    \n                  Kappa : 1         \n                                    \n Mcnemar's Test P-Value : NA        \n                                    \n            Sensitivity : 1.0000    \n            Specificity : 1.0000    \n         Pos Pred Value : 1.0000    \n         Neg Pred Value : 1.0000    \n             Prevalence : 0.4963    \n         Detection Rate : 0.4963    \n   Detection Prevalence : 0.4963    \n      Balanced Accuracy : 1.0000    \n                                    \n       'Positive' Class : Adelgen   \n                                    \n\n\nShow code\npredict_data2<-na.omit(svmL_f$trainingData)\np2<-predict(svmL_f,predict_data2)\nroc(response=predict_data2$.outcome,pred=as.numeric(p2),plot=TRUE, legacy.axes=TRUE)\n\n\n\nCall:\nroc.default(response = predict_data2$.outcome, predictor = as.numeric(p2),     plot = TRUE, legacy.axes = TRUE)\n\nData: as.numeric(p2) in 67 controls (predict_data2$.outcome Adelgen) < 68 cases (predict_data2$.outcome Chinstrap).\nArea under the curve: 0.9926\nShow code\npar(new=TRUE)\nroc(response=predict_data2$.outcome,pre=fitted(logistic_f_model),plot=TRUE,legacy.axes=TRUE,col=2)\n\n\n\n\nCall:\nroc.default(response = predict_data2$.outcome, predictor = fitted(logistic_f_model),     plot = TRUE, legacy.axes = TRUE, col = 2)\n\nData: fitted(logistic_f_model) in 67 controls (predict_data2$.outcome Adelgen) < 68 cases (predict_data2$.outcome Chinstrap).\nArea under the curve: 1\n\n\nShow code\nsvmL_f$results %>%filter(C==svmL_f$bestTune$C)\n\n\n         C  Accuracy     Kappa AccuracySD    KappaSD\n1 6.210526 0.9916667 0.9833333 0.02635231 0.05270463\nShow code\nlogistic_f_model$results\n\n\n  parameter Accuracy     Kappa AccuracySD    KappaSD\n1      none 0.977381 0.9547619 0.03656397 0.07312793\n\n\nPart 4: What the Model Shows?\n\nWith the SVM model obtained in the previous part, we can learn some themes about each classification. Below is a summary of average penguins’ features in each classification. As we can see, both from the summary, Chinstrap penguins tend to have longer and deeper bill compare to Adelgen penguins.\n\n\nAlso, Chinstrap seems to have shorter flippers in general, since the average flipper length for Chinstrap penguins is smaller. Moreover, by looking at the histograms of flipper length, we can also notice that within Adelgen penguins, the length of flippers has a wide range: some have very short flippers while some have very long flippers. Similarly, for body mass, Adelgen penguins tend to have a wider range of body mass while Chinstrap penguins tend to be less heavy in general.\n\n\nBy looking at the summary, all Chinstrap penguins live in Dream island while most Adelgen penguins live in Biscoe island with only a small group living in the other two islands.\n\n\nIn summary, based on the classification model in the previous part, we learn that Chinstrap penguins tend to have longer bill and deeper bill, shorter flipper, lower body mass, and live in Dream island; Adelgen penguins tend to have shorter and shallow bills, diverse length of the flipper and body mass, and mostly live in Biscoe island.\n\n\nShow code\nmy_model <- svm(as.factor(species)~.,new_penguins, kernel='linear',\n              cost=svmL_f$bestTune$C, decision.values=T)\nls=data.frame(predict(my_model,new_penguins))\n\npenguins_with_cluster <- new_penguins %>%\n  mutate(sex=as.numeric(as.factor(sex))-1)%>%\n  na.omit() \npenguins_with_cluster <- merge(penguins_with_cluster,ls, by.x = 0, by.y = 0) %>% rename(predicted=predict.my_model..new_penguins.\n) %>% \n  mutate(predicted=as.factor(predicted)) %>%\n  select(-Row.names)\n\npenguins_with_cluster<-penguins_with_cluster %>% mutate(islandDream=case_when(\n  island=='Dream'~ 1, island!='Dream'~0))%>%\n  mutate(islandBiscoe=case_when(\n  island=='Biscoe'~ 1, island!='Biscoe'~0)) %>%\n  mutate(islandTorgersen = case_when(\n  island=='Torgersen'~ 1,island!='Torgersen'~0))\n\n\n\n\nShow code\npenguins_with_cluster %>% group_by(as.factor(predicted)) %>% summarise_all(mean)\n\n\n# A tibble: 2 x 12\n  `as.factor(predicted)` species island bill_length_mm bill_depth_mm\n  <fct>                    <dbl>  <dbl>          <dbl>         <dbl>\n1 Adelgen                     NA     NA           43.1          16.5\n2 Chinstrap                   NA     NA           48.8          18.4\n# … with 7 more variables: flipper_length_mm <dbl>,\n#   body_mass_g <dbl>, sex <dbl>, predicted <dbl>, islandDream <dbl>,\n#   islandBiscoe <dbl>, islandTorgersen <dbl>\n\n\nShow code\npar(mfrow=c(2,2))\npenguins_with_cluster%>%\n  ggplot(aes(x=bill_depth_mm))+\n  geom_histogram()+\n  labs(title = \"Bill depth by classifications\")+\n  facet_wrap(~as.factor(predicted))\n\n\nShow code\npenguins_with_cluster%>%\n  ggplot(aes(x=bill_length_mm))+\n  geom_histogram()+\n  labs(title = \"Bill length by classifications\")+\n  facet_wrap(~predicted)\n\n\nShow code\npenguins_with_cluster%>%\n  ggplot(aes(x=flipper_length_mm))+\n  geom_histogram()+\n  labs(title = \"Flipper length by classifications\")+\n  facet_wrap(~predicted)\n\n\nShow code\npenguins_with_cluster%>%\n  ggplot(aes(x=body_mass_g))+\n  geom_histogram()+\n  labs(title = \"Body mass by classifications\")+\n  facet_wrap(~predicted)\n\n\n\n\n\nShow code\npenguins_with_cluster%>%\n  ggplot(aes(x=as.factor(sex)))+\n  geom_bar()+\n  labs(title = \"Sex by classifications\")+\n  facet_wrap(~predicted)\n\n\nShow code\npenguins_with_cluster%>%\n  ggplot(aes(x=as.factor(island)))+\n  geom_bar()+\n  labs(title = \"Island by classifications\")+\n  facet_wrap(~predicted)\n\n\n\n\n\nReference:\nJames, G., Witten, D., Hastie, T., & Tibshirani, R. (2017). An introduction to statistical learning with applications in R. New York: Springer.\nCampbell, J. (2020). SVM with caret. Retrieved December 13, 2020, from https://rpubs.com/uky994/593668\n\n\n\n",
    "preview": "projects/ml_p2/pengiuns.png",
    "last_modified": "2021-11-06T15:35:26-05:00",
    "input_file": {}
  },
  {
    "path": "projects/ml_p1/",
    "title": "Machine Leanring Project 1: Explore Price of Airbnb Listings in NYC",
    "description": "In this project, I used a Lasso model to explore influencers of price of Airbnb listing in NYC, evaluated the model, and improved it accordingly.",
    "author": [
      {
        "name": "Rita Liu",
        "url": {}
      }
    ],
    "date": "2020-11-23",
    "categories": [],
    "contents": "\n\nContents\nSetting Up\nPart 1: Process the data\nPart 2: Analyzepreliminary insights of price\nSelecting Variables Using Simple Lasso Model\nIs the model wrong?\nLinear regression using log(price)\n\nPart 3: Summarize\nApendix: GAM Model\n\nSetting Up\n\nShow code\nlibrary(ggplot2)  # for plots\nlibrary(GGally)   # for pairs plots\nlibrary(ggridges) # for joy plots\nlibrary(dplyr)    # for wrangling\nlibrary(caret)    # for machine learning algorithms\nlibrary(stringr)\nairbnb <- read.csv(\"https://www.macalester.edu/~ajohns24/data/NYC_airbnb_kaggle.csv\")\nnbhd <-read.csv(\"https://www.macalester.edu/~ajohns24/data/NYC_nbhd_kaggle.csv\")\n\n\n\n\nPart 1: Process the data\n\nI used NYC Airbnb data on Kaggle, joined two datasets, selected 5000 samples, and mutated varaibles for further use.\n\n\nShow code\nairbnb <- airbnb %>% rename(neighbourhood=neighbourhood_cleansed)\nairbnb <-airbnb %>% left_join(nbhd)\n#Process the data for model input\nset.seed(830)\nairbnb_sub<-airbnb%>%\n  filter(price<=1000) %>%\n  select(-id,-square_feet) %>%\n  sample_n(5000) %>%\n  mutate(\n    noamenities = str_count(amenities, ',') +1, # count the number of amenities in each house\n    host_response_rate = na.pass(as.numeric(str_remove(host_response_rate,\"%\"))), # convert host_response_rate from string to number\n    host_response_time=as.factor(host_response_time), # prepare all categorical variables for the model to use\n    host_is_superhost=as.factor(host_is_superhost),\n    neighbourhood=as.factor(neighbourhood),\n    property_type=as.factor(property_type),\n    room_type=as.factor(room_type),\n    bed_type=as.factor(bed_type),\n    instant_bookable = as.factor(instant_bookable),\n    cancellation_policy =as.factor(cancellation_policy),\n    calendar_updated=as.factor(calendar_updated),\n    room_type=as.factor(room_type),\n    is_location_exact=as.factor(is_location_exact),\n    host_has_profile_pic=as.factor(host_has_profile_pic),\n    is_business_travel_ready=as.factor(is_business_travel_ready),\n    require_guest_profile_picture=as.factor(require_guest_profile_picture),\n    neighbourhood_group=as.factor(neighbourhood_group)\n    ) %>%\n  select(-amenities)\n\n\n\nPart 2: Analyze\npreliminary insights of price\n\nShow code\nggplot(airbnb_sub,aes(x=price)) + \n  geom_histogram()\n\n\n\n\n\n\nAbove shows the distribution of listing price of Airbnb in our sample. The price of the Airbnb skews heavily to the right and a typical price for a Airbnb housing in the New York is around 125 dollars per night. The price for housing spreads out from 0 to 1000 dollars and is relatively disperse. There are probably a few outliers at the high end of the price.\n\n\nSelecting Variables Using Simple Lasso Model\n\nTo build and evaluate a predictive model of listing price, I first decide to use the LASSO model to choose the predictors I wish to include in the model since including all the variables available would generate a too complicated model for our purpose and the model tends to overfit. Lasso is chosen over the variable selection method not only because it is more computationally efficient, but also because it does not perform multiple testings when selecting the model so our results would not be overestimated.\n\n\n\nShow code\nset.seed(830)\n# Perform LASSO\nlasso_model <- train(\n    price ~ .,\n    data = airbnb_sub,\n    method = \"glmnet\",\n    tuneGrid = data.frame(alpha = 1, lambda = seq(0, 10, length = 100)),\n    trControl = trainControl(method = \"cv\", number = 10, selectionFunction = \"oneSE\"),\n    metric = \"MAE\",\n    na.action = na.omit\n)\n#coef(lasso_model$finalModel,lasso_model$bestTune$lambda)\n\n\n\n\nI have chosen the method “oneSE” over “best” because in this case, I want to have a relatively simple model. Here, lambda equals to 5.252525. The plot showing relationship between tuning parameter and MAE shows below. As we can see, the MAE for using a tuning parameter of 5.252525 is close to the lowest MAE. Therefore, this lambda value is reasonable. The variables remained after shrinkage are longitude, room_type, accommodates, bathrooms, bedrooms, availability_30, review_scores_rating, is_business_travel_ready, reviews_per_month, noamenities, and guests_included. Note that a few categorical variables like neighborhood and neighbourhood_group are removed because not all the levels remained after the shrinkage.\n\n\nShow code\nlasso_model$bestTune\n\n\n   alpha   lambda\n53     1 5.252525\nShow code\nplot(lasso_model)\n\n\n\n\n\nI then fit a linear regression using the variables selected by the LASSO model. On average, if we apply this model to a new set of data, a typical error would be 45.40 dollars. Here, R^2 is relatively low: only 53.97% of the listing price is explained by the model. I further study the residual plot to exam if the model is wrong.\n\n\nShow code\nmy_model <- train(\n    price ~ longitude+room_type + accommodates + bathrooms + bedrooms + availability_30+review_scores_rating+is_business_travel_ready+reviews_per_month+noamenities+guests_included,\n    data = airbnb_sub,\n    method = 'lm',\n    trControl = trainControl(method = 'cv', number = 10),\n    metric = \"MAE\",\n    na.action = na.omit\n)\n\n\n\n\nShow code\nsummary(my_model)\n\n\n\nCall:\nlm(formula = .outcome ~ ., data = dat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-307.27  -34.68   -5.84   23.35  774.77 \n\nCoefficients:\n                            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)               -4.340e+04  2.092e+03 -20.746  < 2e-16 ***\nlongitude                 -5.864e+02  2.828e+01 -20.738  < 2e-16 ***\n`room_typePrivate room`   -6.581e+01  2.984e+00 -22.054  < 2e-16 ***\n`room_typeShared room`    -9.311e+01  7.777e+00 -11.972  < 2e-16 ***\naccommodates               1.558e+01  1.062e+00  14.677  < 2e-16 ***\nbathrooms                  4.132e+01  3.348e+00  12.343  < 2e-16 ***\nbedrooms                   1.736e+01  2.321e+00   7.478 9.28e-14 ***\navailability_30            1.684e+00  1.432e-01  11.760  < 2e-16 ***\nreview_scores_rating       7.559e-01  1.486e-01   5.086 3.84e-07 ***\nis_business_travel_readyt  7.710e+00  4.522e+00   1.705   0.0883 .  \nreviews_per_month         -3.553e+00  7.441e-01  -4.775 1.86e-06 ***\nnoamenities                4.576e-01  1.800e-01   2.543   0.0110 *  \nguests_included            5.305e+00  1.255e+00   4.227 2.42e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 72.22 on 3835 degrees of freedom\nMultiple R-squared:  0.5397,    Adjusted R-squared:  0.5382 \nF-statistic: 374.7 on 12 and 3835 DF,  p-value: < 2.2e-16\n\n\nShow code\nmy_model$results\n\n\n  intercept     RMSE  Rsquared      MAE   RMSESD RsquaredSD    MAESD\n1      TRUE 72.36787 0.5396401 45.40293 8.248047 0.04084573 2.843008\n\nIs the model wrong?\n\nShow code\n# Combine residuals & predictions into data frame\nresult_df <- data.frame(resid = resid(my_model), fitted = fitted(my_model))\n\n# Residual plot\nggplot(result_df, aes(x = fitted, y = resid)) + \n  geom_point() + \n  geom_hline(yintercept = 0)\n\n\n\n\n\n\nAbove shows the residual plot of a simple regression model with selected predictors. The residual plot is not ideal because it is not balanced above and below zero. We can also observe a negative trend of the residuals. The plot is also heteroskedastic: the smaller the fitted value, the smaller the residuals are. One possible reason is that our outcome price skews heavily to the right. Therefore, I even the distribution of outcome by using log(price).\n\nLinear regression using log(price)\npreliminary insights of log(price)\n\nBelow is the graph showing distribution of log(price). As we can see, it is about symmetric and centered around 5. Using log(price) should help to generate a better model.\n\n\nShow code\nggplot(airbnb_sub,aes(x=log(price))) + \n  geom_histogram()\n\n\n\n\n\nActual Model: log(price) as the outcome\n\nShow code\nlog_model_data<-airbnb_sub %>%\n  select(price,longitude,room_type,accommodates,bathrooms,bedrooms,availability_30,review_scores_rating,is_business_travel_ready,reviews_per_month,noamenities,guests_included) %>%\n  mutate(logprice=log(price))%>%\n  select(-price) %>%\n  filter(logprice>-Inf)\n\n\n\n\nShow code\nlog_model <- train(\n    logprice ~ longitude+room_type + accommodates + bathrooms + bedrooms + availability_30+review_scores_rating+is_business_travel_ready+reviews_per_month+noamenities+guests_included,\n    data = log_model_data,\n    method = 'lm',\n    trControl = trainControl(method = 'cv', number = 10),\n    metric = \"MAE\",\n    na.action = na.omit\n)\n\n\n\n\nShow code\nlog_model$results\n\n\n  intercept      RMSE  Rsquared       MAE     RMSESD RsquaredSD\n1      TRUE 0.3860566 0.6310932 0.2914568 0.01727447 0.02462859\n       MAESD\n1 0.01570924\n\n\nShow code\n# Combine residuals & predictions into data frame\nresult_df3 <- data.frame(resid = resid(log_model), fitted = fitted(log_model))\n\n# Residual plot\nggplot(result_df3, aes(x = fitted, y = resid)) + \n  geom_point() + \n  geom_hline(yintercept = 0)\n\n\n\n\n\n\nWith the log model, R^2 improved: now 63.11% of y variable is explained by the model. The residual plot also improved: the residuals are now balanced around zero and seem to be randomly distributed. The residual plot is not heteroskedastic anymore. Therefore, log model probably is a better model in predicting the price.\n\n\n\nThrough the model building process, I choose the least square regression over the KNN and GAM model because the least square model is easier to interpret. However, the GAM model is indeed better if the relationship between the outcome and predictors are nonlinear. Therefore, I build a gam model using the same predictors selected by the LASSO model. The detailed codes and results can be found in the appendix. The gam model has an MAE of 43.71 dollars and a r^2 of 56.50%, not significantly outperform the least square regression. As a result, I choose the least square model using log(price) as the outcome.\n\n\nPart 3: Summarize\n\nShow code\nsummary(log_model)\n\n\n\nCall:\nlm(formula = .outcome ~ ., data = dat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.4487 -0.2395 -0.0009  0.2234  2.4892 \n\nCoefficients:\n                            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)               -3.024e+02  1.117e+01 -27.065  < 2e-16 ***\nlongitude                 -4.142e+00  1.510e-01 -27.427  < 2e-16 ***\n`room_typePrivate room`   -5.852e-01  1.593e-02 -36.739  < 2e-16 ***\n`room_typeShared room`    -1.016e+00  4.149e-02 -24.483  < 2e-16 ***\naccommodates               7.393e-02  5.667e-03  13.046  < 2e-16 ***\nbathrooms                  1.080e-01  1.786e-02   6.048 1.60e-09 ***\nbedrooms                   5.886e-02  1.240e-02   4.746 2.15e-06 ***\navailability_30            8.692e-03  7.639e-04  11.377  < 2e-16 ***\nreview_scores_rating       5.618e-03  7.930e-04   7.084 1.66e-12 ***\nis_business_travel_readyt  3.051e-02  2.413e-02   1.264    0.206    \nreviews_per_month         -1.623e-02  3.970e-03  -4.088 4.45e-05 ***\nnoamenities                3.965e-03  9.604e-04   4.129 3.73e-05 ***\nguests_included            3.560e-02  6.695e-03   5.317 1.12e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3853 on 3832 degrees of freedom\nMultiple R-squared:  0.6332,    Adjusted R-squared:  0.6321 \nF-statistic: 551.3 on 12 and 3832 DF,  p-value: < 2.2e-16\nShow code\nround(exp(log_model$finalModel$coefficients),3)\n\n\n              (Intercept)                 longitude \n                    0.000                     0.016 \n  `room_typePrivate room`    `room_typeShared room` \n                    0.557                     0.362 \n             accommodates                 bathrooms \n                    1.077                     1.114 \n                 bedrooms           availability_30 \n                    1.061                     1.009 \n     review_scores_rating is_business_travel_readyt \n                    1.006                     1.031 \n        reviews_per_month               noamenities \n                    0.984                     1.004 \n          guests_included \n                    1.036 \n\n\n\nAirbnb price is positively associated with the following factors: the number of accommodates, bathrooms, bedrooms, amenities, and guests included, as well as its availability in 30 days, whether it is ready for business travel, and the rating of review scores. Among them, the number of accommodates and bathrooms have the strongest and significant impact. With one more accommodate and bathroom, the price on average would be 1.077 and 1.114 times higher respectively, holding other variables constant.\n\n\n\nOn the other hand, longitude, private room, shared room, and reviews per month are negatively associated with price. Among them, the strongest and the most significant predictors to listing price seems to be longitude and room types. If the longitude increases by 1 degree, the price of Airbnb will, on average, be 0.016 times less, holding other variables constant. For room types, compared to entire home or apartment, if the room types are private rooms or shared rooms, the price, on average, would be 0.557 and 0.362 times lower respectively, holding other variables constant. Below is the graph picturing the relationship between room types and price. According to the box plot, it seems that private rooms and shared rooms typically are cheaper than the entire home or apartment.\n\n\nShow code\nggplot(airbnb_sub,aes(y=price,x=room_type)) +\n  geom_boxplot()\n\n\n\n\n\nApendix: GAM Model\n\nShow code\n    ggplot(airbnb_sub,aes(y=price,x=bathrooms))+\n      geom_point()\n\n\n\n\n\n\nI do notice that not all the variables have linear relationship with the outcome. For example, above is the plot showing the relationship between the number of bathrooms and the price. Clearly, this is not a linear relationship. In this case, GAM might be a better model when using to predict the outcome. Therefore, I fitted a GAM model using the same set of predictors.\n\n\nShow code\n#process data\ngam_model_data <- airbnb_sub %>% \n      select(price,longitude,room_type,accommodates,bathrooms,bedrooms,availability_30,number_of_reviews,review_scores_rating,is_business_travel_ready,reviews_per_month,noamenities,guests_included) \n     \ngam_model_data <- data.frame(model.matrix(~ . - 1, gam_model_data))\n    \ngam_model <- train(\n    price ~ longitude+room_typeEntire.home.apt+room_typePrivate.room+room_typeShared.room+accommodates+bathrooms+bedrooms+availability_30+number_of_reviews+review_scores_rating+is_business_travel_readyt+reviews_per_month+noamenities+guests_included,\n    data = gam_model_data,\n    method = 'gamLoess',\n    tuneGrid = data.frame(span = seq(0.1, 1, length = 10), degree = 1),\n    trControl = trainControl(method = \"cv\", number = 10, selectionFunction = \"best\"),\n    metric = \"MAE\",\n    na.action = na.omit\n)\n\ngam_model$results %>% filter(span==gam_model$bestTune$span)\n\n\n  span degree    RMSE  Rsquared      MAE   RMSESD RsquaredSD    MAESD\n1  0.7      1 70.1644 0.5650253 43.71761 8.889075 0.05344549 3.331914\n\n\nWith the GAM model, the R^2 is 0.565, meaning that 56.5% listing price are explained by the model; The MAE is 43.71 dollars: when we apply this model to a new set of data, we would expect the prediction to be off by 43.84 dollars. These results are not better than the least sqaure model.\n\n\n\nShow code\n# Combine residuals & predictions into data frame\nresult_df2 <- data.frame(resid = resid(gam_model), fitted = fitted(gam_model))\n\n# Residual plot\nggplot(result_df2, aes(x = fitted, y = resid)) + \n  geom_point() + \n  geom_hline(yintercept = 0)\n\n\n\n\n\n\nAbove shows the residual plot of GAM model. The residual plot shows that this model might be wrong too since it is not balanced aroudn zero and the residuals are not random.\n\n\n\n\n",
    "preview": "projects/ml_p1/airbnb.png",
    "last_modified": "2021-11-06T15:00:32-05:00",
    "input_file": {}
  }
]
